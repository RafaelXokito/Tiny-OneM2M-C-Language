%% 
%% Copyright 2019-2020 Elsevier Ltd
%% 
%% This file is part of the 'CAS Bundle'.
%% --------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'CAS Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for cas-dc documentclass for 
%% double column output.

%\documentclass[a4paper,fleqn,longmktitle]{cas-dc}
\documentclass[a4paper,fleqn]{cas-dc}

%\usepackage[authoryear,longnamesfirst]{natbib}
%\usepackage[authoryear]{natbib}
\usepackage[numbers]{natbib}

%%%Author definitions
\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}
\tsc{EP}
\tsc{PMS}
\tsc{BEC}
\tsc{DE}
%%%
% -------------------------------------------------------------------
% Pacotes para inserção de figuras e subfiguras
\usepackage{subfig,epsfig,tikz,float}		            % Packages de figuras. 
\usepackage{graphicx}
\graphicspath{ {./figs/} }
% -------------------------------------------------------------------
% \usepackage{amssymb}
% -------------------------------------------------------------------
% Pacotes para inserção de tabelas
\usepackage{booktabs,multicol,multirow,tabularx,array}          % Packages para tabela
\usepackage{natbib}
\usepackage{pifont}
\usepackage{xcolor}
\usepackage{algpseudocode}
\usepackage{algorithm}
% -------------------------------------------------------------------
\PassOptionsToPackage{style=super,nolist}{glossaries}
\PassOptionsToPackage{acronym}{glossaries}
\PassOptionsToPackage{nonumberlist}{glossaries}
\usepackage{glossaries}
\newacronym{ai}{AI}{Artificial Intelligence}
\newacronym{iot}{IoT}{Internet of Things}
\newacronym{iomt}{IoMT}{Internet of Medical Things}
\newacronym{mtc}{MTC}{Machine Type Communication}
\newacronym{m2m}{M2M}{Machine To Machine}
\newacronym{tls}{TLS}{Transport Layer Security}
\newacronym{dtls}{DTLS}{Datagram Transport Layer Security}
\newacronym{iotdm}{IoTDM}{Internet of Things Device Management}
\newacronym{sdn}{SDN}{Software-Defined Networking}
\newacronym{http}{HTTP}{Hypertext Transfer Protocol}
\newacronym{coap}{CoAP}{Constrained Application Protocol}
\newacronym{mqtt}{MQTT}{Message Queuing Telemetry Transport}
\newacronym{rtt}{RTT}{Round Trip Time}
\newacronym{ap}{AP}{Access Points}
\newacronym{rsus}{RSUs}{Roadside Units}
\newacronym{qos}{QoS}{Quality of Service}
\newacronym{ccas}{CCAS}{Cluster-based Congestion-mitigating Access Scheme}
\newacronym{mtcds}{MTCDs}{Machine-type Communication Devices}
\newacronym{mtcd}{MTCD}{Machine-type Communication Device}
\newacronym{bs}{BS}{Base Station}
\newacronym{mtcg}{MTCG}{Machine-type Communication Gateway}
\newacronym{ca}{CA}{Certificate Authority}
\makeglossaries
% -------------------------------------------------------------------
\usepackage[utf8]{inputenc} % The default since 2018
\DeclareUnicodeCharacter{200B}{{\hskip 0pt}}
% -------------------------------------------------------------------
\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}
\shorttitle{Leveraging social media news}
\shortauthors{CV Radhakrishnan et~al.}

\title [mode = title]{Tiny oneM2M C Language API}                     

\credit{Conceptualization of this study, Methodology, Software}

\author[1]{Rafael Pereira}[type=editor,
                        %auid=000,bioid=1,
                        linkedin='rafaelmendespereira',
                        orcid=0000-0001-8313-7253]
%\cormark[1]
%\fnmark[1]
\ead{rafael.m.pereira@ipleiria.pt}
   
\address[1]{Computer Science and Communications Research Centre, School of Technology and Management, Polytechnic of Leiria, 2411-901 Leiria, Portugal}

\begin{abstract}
This template helps you to create a properly formatted \LaTeX\ manuscript.

\noindent\texttt{\textbackslash begin{abstract}} \dots 
\texttt{\textbackslash end{abstract}} and
\verb+\begin{keyword}+ \verb+...+ \verb+\end{keyword}+ 
which
contain the abstract and keywords respectively. 

\noindent Each keyword shall be separated by a \verb+\sep+ command.
\end{abstract}

%\begin{graphicalabstract}
%\includegraphics{figs/grabs.pdf}
%\end{graphicalabstract}
%
%\begin{highlights}
%\item Research highlights item 1
%\item Research highlights item 2
%\item Research highlights item 3
%\end{highlights}

\begin{keywords}
quadrupole exciton \sep polariton \sep \WGM \sep \BEC
\end{keywords}


\maketitle

\section{Introdution}

The \gls{iot} has experienced tremendous growth in recent years, with an ever-increasing number of devices being connected and integrated into various applications, such as smart cities, industrial automation, and environmental monitoring. One key aspect enabling this widespread adoption is \gls{mtc}, which facilitates seamless communication between machines and devices without human intervention. \gls{mtc} allows for efficient data exchange among heterogeneous devices, leading to better coordination and improved overall system performance.

In this context, interoperability plays a vital role in ensuring the seamless functioning of \gls{iot} ecosystems. As the number and diversity of connected devices continue to grow, so does the need for a robust mechanism to manage communication between devices employing different communication protocols, data formats, and standards. \gls{mtc} gateways serve as an essential bridge between these devices and the network infrastructure, enabling effective data routing and ensuring smooth communication among heterogeneous devices.

Existing \gls{mtc} gateway implementations predominantly rely on high-level programming languages, which offer the advantages of rapid development cycles and ease of use. However, these high-level languages often come with trade-offs, such as limited control over hardware resources, increased memory overhead, and suboptimal performance. These limitations can pose significant challenges when deploying large-scale \gls{iot} systems, where efficient resource utilization, interoperability, and high-throughput data transmission are essential.

In this paper, we present a novel \gls{mtc} gateway implementation using a low-level programming language to address the limitations of high-level language-based implementations. Our approach is designed to deliver superior performance, reduced memory overhead, and increased control over hardware resources, while maintaining compatibility with a wide range of \gls{mtc} protocols. By focusing on key challenges associated with \gls{mtc} gateways, such as protocol interoperability, scalability, and latency, our implementation aims to provide a robust and efficient solution for the evolving \gls{iot} landscape.

\textcolor{red}{Paper structure} The remainder of this paper is organized as follows: Section \ref{relatedWork} provides a comprehensive review of the related work in \gls{mtc} gateway implementations, discussing various programming languages and architectural choices, as well as the importance of interoperability in \gls{iot} systems. Section \ref{architecture} delves into the specific challenges associated with \gls{mtc} protocol integration and outlines the design principles of our novel low-level language-based gateway. Section \ref{tests} presents the experimental setup and performance evaluation of our proposed solution. Finally, Section \ref{conclusion} concludes the paper and highlights potential future research directions.

\section{Related work}
\label{relatedWork}
\gls{mtc} has become an increasingly critical aspect of modern communication systems, driving the transformation of various applications in the ever-evolving \gls{iot} ecosystem. \gls{mtc} enables the seamless interaction between a vast array of devices, ranging from wearables and smartphones to industrial equipment and smart home appliances, facilitating the data exchange required for intelligent and automated decision-making processes. With the exponential growth of \gls{iot} devices and their stringent requirements for low latency, high reliability, and energy efficiency, the implementation of \gls{mtc} in low-level languages has emerged as a promising solution to address these challenges.

In this section, we will provide an overview of the current state-of-the-art in \gls{mtc} gateway implementations, highlighting the advantages and disadvantages of various programming languages and architectural choices. Additionally, we will explore the specific challenges associated with \gls{mtc} protocol integration and discuss how our novel approach addresses these issues to provide a comprehensive solution for the next generation of \gls{iot} deployments.

Rubi et al. \cite{Rubi2019} propose an \gls{iomt} platform with a cloud-based electronic health system that collects data from various sources and relays it through gateways or fog servers. The platform enables data management, visualization of electronic records, and knowledge extraction through big data processing, machine learning, and online analytics processing. Additionally, it offers data sharing services for third-party applications, improving healthcare services and interoperability.

The authors in \cite{Volkov2017} conducted \gls{iotdm} service testing for smart city management, focusing on the ecological situation in Saint-Petersburg's central district, using an \gls{sdn} network infrastructure. They developed a hierarchical model and \gls{iot} traffic generator in compliance with oneM2M specifications and implemented \gls{http}, \gls{coap}, and \gls{mqtt} protocols. Their findings suggested that \gls{http} is the most suitable protocol for registering \gls{iot} devices to the \gls{iotdm} service, while \gls{mqtt} is optimal for transmitting sensor data. The authors concluded that the \gls{sdn} approach significantly reduces the \gls{rtt} parameter, demonstrating its potential in managing \gls{iot} traffic in smart city environments.

Soumya et al. \cite{Datta2015} explore Fog Computing as a consumer-centric \gls{iot} service deployment platform, particularly for connected vehicles and intelligent transportation systems requiring low latency, high mobility, real-time data analytics, and wide geographic coverage. They present an \gls{iot} architecture for connected vehicles, integrating Fog Computing platforms into oneM2M standard architecture. The architecture comprises virtual sensing zones, \gls{ap}, \gls{rsus}, \gls{m2m} gateways, and cloud systems. Fog Computing enables consumer-centric services such as \gls{m2m} data analytics with semantics, discovery of \gls{iot} services, and management of connected vehicles. The paper highlights the advantages of Fog Computing, including reduced latency, improved \gls{qos}, real-time data analysis, and actuation for superior user experience and consumer-centric \gls{iot} products.

This paper \cite{Liang2018} presents a \gls{ccas} that addresses the issue of collision and access efficiency in \gls{m2m} communications. The authors propose a modified spectral clustering algorithm to form clusters based on the location and service requirements of \gls{mtcds}, ensuring that devices with similar requirements are grouped together. They also develop an \gls{mtcg} selection algorithm that considers the delay requirements of \gls{mtcds} and selects an \gls{mtcg} with the appropriate forwarding threshold. The proposed CCAS divides the data transmission process into an \gls{mtcd}-\gls{mtcg} stage and an \gls{mtcg}-\gls{bs} stage to reduce collision probability and increase the number of successfully received packets, without causing additional average access delays for \gls{mtcds}. The paper includes a theoretical analysis of the \gls{mtcg} aggregation and forwarding process, as well as performance evaluations through simulations.

The paper \cite{Thielemans2019} proposes a resource-efficient secure implementation of OneM2M based on C programming language for \gls{iot}, while also performing an analysis of the OneM2M architecture and protocols. Furthermore, the authors highlight the relevance of a lightweight implementation using optimized data structures and algorithms for \gls{iot} devices with limited resources and propose a solution that can reduce computational costs and processing time. Secure communication protocols such as \gls{tls} and \gls{dtls}, as well as authentication and encryption mechanisms, were used to ensure security. The authors also suggested that the cryptography algorithms chosen must be lightweight to reduce the computational burden on \gls{iot} devices. The evaluation of this implementation meant using a set of benchmarks while comparing the developed OneM2M implementation with existing ones. Results showed that, in terms of memory usage and processing time, the author's proposed implementation of OneM2M using C language is more resource-efficient.

The authors in \cite{Benoygopal2021} present a secure authentication and access control scheme for OneM2M-based \gls{iot} systems. Moreover, propose an approach to address the challenge of identity and access management in large-scale \gls{iot} systems. The proposed scheme is based on the role-based access control model, where system administrators can define access policies based on the roles of the users. Their scheme entails a secure authentication and authorization mechanism based on certificates, which are issued by a trusted \gls{ca}. The evaluation process used a prototype implementation of the OneM2M standard and compared it with existing access control schemes. Therefore concluding that their scheme achieves better performance and scalability while providing a flexible and fine-grained access control mechanism. However, the proposed scheme contains security and privacy issues for which the authors presented several countermeasures to mitigate them.

The paper \cite{Kim2018} discusses the design, implementation and evaluation of an interoperable platform for the Internet of Things \gls{iot} using the OneM2M standard, where the authors focused on implementing the standard using C language. Moreover, the authors discussed how their C implementation handles registration, discovery and subscription-resource management tasks while proposing optimizations to improve performance. Experiments were designed to assess the scalability, reliability, and processing time with different loads and data sizes. Those experiments concluded that the C implementation of OneM2M achieved better performance than other implementations regarding response time and memory usage. Regarding security, the paper also details oneM2M's security features, namely, authentication, authorization, and confidentiality, how the C implementation handles these features and how to improve the platform's security. Interoperability features, such as standardized interfaces and protocols, and their C implementation were also detailed in the paper since one of oneM2M's main objects must be the interoperability between different \gls{iot} platforms and devices.


% https://www.scopus.com/results/results.uri?sort=plf-f&src=s&st1=%28onem2m+OR+mtc%29+and+%22API%22&sid=2e6f61fce202fa9e6635e553544fee3c&sot=b&sdt=b&sl=40&s=TITLE-ABS-KEY%28%28onem2m+OR+mtc%29+and+%22API%22%29&origin=searchbasic&editSaveS=&yearFrom=Before+1960&yearTo=Present

\section{Arquitecture}
\label{architecture}

TinyOneM2M, a compact, cross-platform, and user-friendly implementation of the OneM2M standard, boasts a meticulously designed architecture that adapts seamlessly to both centralized and decentralized modes of operation. Designed primarily to enable decentralized, peer-to-peer applications, TinyOneM2M stands out due to its light footprint and high adaptability.

Reflecting on the high-level architecture of TinyOneM2M, as depicted in Figure \ref{fig:highlevelarch}, the software is installed on a hardware system and communicates with other entities using HTTP and MQTT (for subscription notifications) protocols. This flexible architecture allows TinyOneM2M to integrate seamlessly into existing infrastructure, establishing efficient communication and data transfer between nodes.

\begin{figure}[htbp]
\centering
\includegraphics[width=7cm]{HightLevel}
\caption{High Level Architecture of TinyOneM2M}
\label{fig:highlevelarch}
\end{figure}

TinyOneM2M employs a set of resources for its operations: Common Services Entity Base (CSE\_Base), Application Entity (AE), Container (CNT), Content Instance (CIN), and Subscription (SUB). These resources provide a rich set of capabilities to TinyOneM2M, forming a foundational part of its functional landscape.

Central to TinyOneM2M's functioning is its resource hierarchy, which sets the structural blueprint for resource organization and management. This hierarchy, illustrated in Figure \ref{fig:hierarchy}, starts with the CSE\_Base as the root node, established during API startup. The CSE\_Base encapsulates AE, CNT, and SUB resources. AE can contain CNT and SUB resources, while CNT can house additional CNT, CIN, and SUB resources. CIN, being read-only and supporting only GET and DELETE requests, does not contain any resources. SUB resources, on the other hand, can contain additional SUB resources.

\begin{figure}[htbp]
\centering
\includegraphics[width=7cm]{Hierarchy}
\caption{Resource Hierarchy in TinyOneM2M}
\label{fig:hierarchy}
\end{figure}

Each instance of TinyOneM2M manages its own database, which ensures data persistence and isolation. This design approach provides robust data management and scalability, allowing each node to manage its resources efficiently.

Examining the operational scenarios of TinyOneM2M, it is important to contrast the centralized and decentralized approaches. In a centralized scenario, there exists a single instance of TinyOneM2M orchestrating all communication. Each hardware node communicates with this central point to acquire and persist information. This scenario ensures data consistency and simplifies management at the expense of potential communication bottlenecks.

On the other hand, the decentralized scenario leverages peer-to-peer interactions. Each system node hosts a TinyOneM2M instance, enabling it to communicate directly with every other node. This not only enhances scalability but also optimizes data transfer by eliminating the need for a central point of communication. Figure \ref{fig:comparison} highlights these differences between the two scenarios.

\begin{figure}[htbp]
\centering
\includegraphics[width=7cm]{CentralizedvsDecentralizedScenario}
\caption{Comparison between Centralized and Decentralized Scenarios in TinyOneM2M}
\label{fig:comparison}
\end{figure}

\section{Proposed Solution}

\section{Tests}
\label{tests}

In the following section we present a series of test cases to evaluate the proposed solution's performance, compatibility, and efficiency. These tests cover various aspects, including the impact of different handling methods for the "expiration time" field in database queries, a comparison of our solution with OpenMTC, cross-platform evaluations on Ubuntu and macOS systems, and compatibility with minimal hardware configurations like the ESP32. By conducting these tests, we aim to provide a comprehensive assessment of the proposed solution's strengths and limitations, allowing for further optimization and refinement to better address the needs of a wide range of applications and use cases.

The Table \ref{tab:machinesSpecs} presents the detailed hardware, operating system, compiler, and library configurations for the machines used for tests. This information is essential for understanding and recreating the testing environment, as well as providing context to the results obtained from the diverse test cases conducted.

\begin{table}[h]
	\scriptsize
	%\makebox[\textwidth]{
	\caption{Hardware, Operating System, Compiler, and Library Specifications for Test Bed Machines.}
	\label{tab:machinesSpecs}
	\begin{tabular}{>{\bfseries}p{3cm} p{4cm}}
	\toprule
	\multicolumn{2}{c}{\textbf{Machine1}} \\
	\toprule
	\toprule
	\multicolumn{2}{c}{\textbf{Hardware}} \\
	\midrule
	Chip                & Apple M1 Pro @ 3.2 GHz \\
	Total Number of Cores & 10 (8 performance and 2 efficiency) \\
	RAM Memory          & 16 GB \\
	\midrule
	\multicolumn{2}{c}{\textbf{Operating System}} \\
	\midrule
	Product Name         & macOS \\
	Product Version      & 12.6 \\
	Kernel Version        & Darwin 21.6.0 \\
	\midrule
	\multicolumn{2}{c}{\textbf{Compiler}} \\
	\midrule
	GCC Version         & Apple clang version 14.0.0 (clang-1400.0.29.102) \\
	\midrule
	\multicolumn{2}{c}{\textbf{Packages}} \\
	\midrule
	SQLite3 Version     & 3.39.3 \\
	\bottomrule
	\bottomrule
	
	\toprule
	\multicolumn{2}{c}{\textbf{Machine2}} \\
	\toprule
	\toprule
	\multicolumn{2}{c}{\textbf{Hardware}} \\
	\midrule
	Chip                & Intel(R) Core(TM)2 CPU 6600 @ 2.40GHz \\
	Total Number of Cores & 2 \\
	RAM Memory          & 8 GB \\
	\midrule
	\multicolumn{2}{c}{\textbf{Operating System}} \\
	\midrule
	Product Name         & Ubuntu \\
	Product Version      & 20.04.5 \\
	Kernel Version        & 5.15.0-67-generic \\
	\midrule
	\multicolumn{2}{c}{\textbf{Compiler}} \\
	\midrule
	GCC Version         & Ubuntu 9.4.0-1ubuntu1~20.04.1 \\
	\midrule
	\bottomrule
	\bottomrule
	
	\toprule
	\multicolumn{2}{c}{\textbf{Machine3}} \\
	\toprule
	\toprule
	\multicolumn{2}{c}{\textbf{Hardware}} \\
	\midrule
	Chip                & Quad-Core Intel Core i7 @ 2,2 GHz \\
	Total Number of Cores & 4 \\
	RAM Memory          & 16 GB \\
	\midrule
	\multicolumn{2}{c}{\textbf{Operating System}} \\
	\midrule
	Product Name         & macOS \\
	Product Version      & 12.6.3 \\
	Kernel Version       & Darwin 21.6.0 \\
	\midrule
	\multicolumn{2}{c}{\textbf{Compiler}} \\
	\midrule
	GCC Version         & Apple clang version 14.0.0 (clang-1400.0.29.202) \\
	\midrule
	\multicolumn{2}{c}{\textbf{Packages}} \\
	\midrule
	SQLite3 Version     & 3.37.0 \\
	\bottomrule
	\bottomrule
	\end{tabular}
\end{table}

To thoroughly evaluate the effectiveness and applicability of our proposed solution, we have designed a comprehensive set of test cases. The first test case aims to evaluate the impact of implementing expiration time field conditions in database queries. By comparing the performance of three different scenarios, we can assess the time cost of each approach and determine which is most effective. The second test case focuses on a comparative analysis of latency for 500 AE resource CRUD operations using the proposed solution and OpenMTC. This will enable us to gauge the relative efficiency of our approach, highlighting its strengths and areas for improvement. The third test case involves performing a cross-platform evaluation through installation and compilation on Ubuntu and macOS systems, ensuring that our solution is versatile and can be seamlessly integrated into different operating environments. In the fourth test case, we will further compare our solution with OpenMTC by measuring the time efficiency of 500 resource discovery requests when the database has 500, 1000, and 2000 records. This test will provide insight into the performance and scalability of our solution in comparison to OpenMTC. Finally, testing the compatibility of our solution with minimal hardware, such as the ESP32 case study, will help verify its suitability for resource-constrained scenarios, broadening its potential applications and enabling its adoption in a wider range of use cases. By conducting these test cases, we will gain a comprehensive understanding of the effectiveness and applicability of our proposed solution, enabling us to refine and optimize it to meet the evolving needs and challenges of the oneM2M standard.

\subsection{Assessing the Efficiency of Meeting Expiration Time Field Objectives in Database Queries on Performance}

In this section, we focus on a specific test case that aims to assess the efficiency of meeting expiration time field objectives in database queries on performance, using codebase version 1.1. The database contains 2000 records in the "mtc" table and 41,000 in the "multivalue" table, adding difficulty to the SELECT queries. The "expiration time" field's purpose is to ensure that resources are deleted or become unused after their expiration. The test will be performed on a "discovery" request with five parameters: ty (resource type), lbl (labels), expireAfter using the "expiration time" field, expireBefore using the "expiration time" field, and filterOperation. It is important to note that the "labels" attribute is an array and its search requires more effort because it searches in the "multivalue" table. By default, the "discovery" request will have a limit of 50 rows listed, which can be adjusted as a parameter, but for this test, the default value will be used. As the limit value increases, the query's latency may grow exponentially, making it crucial to optimize the database and query structure. Three distinct approaches were considered, with the first one being the primary focus of this study:

\begin{itemize}
	\item Selective Query Filtering - This approach filters out expired records by adding a condition to consultation, update, and removal queries to apply only to records with a valid "expiration time" field. A REST route is created to delete invalid records. This method is advantageous because it minimizes the overhead associated with constantly checking and deleting expired records while still maintaining data integrity. The performance of this approach will be compared with two other scenarios in terms of query time.
	\item On-demand Record Deletion - In this approach, all requests use the current context to search for records whose "expiration time" field has already passed and, if so, deletes them. This method adds complexity to each request, potentially impacting performance and increasing the likelihood of implementation errors.
	\item Periodic Expiration Check - A thread is created that remains active indefinitely, checking every X time if there are fields with an exceeded "expiration time". However, there is a chance that expired records would still be present when queries are made on the main threads. This approach can lead to additional resource consumption and potential race conditions, making it less ideal than the first approach.
\end{itemize}

The test case under examination focuses on the first approach, "Selective Query Filtering," and examines its performance in three different scenarios using codebase version 1.1: the codebase with the "expiration time" field as a condition in database queries, the codebase with the "expiration time" field as a condition in database queries when it is indexed in the database, and the original codebase without the "expiration time" field as a condition. The test will be performed by executing 500 "discovery" requests with five parameters in each of the three scenarios on "Machine 1", as shown in Table \ref{tab:machinesSpecs}. By evaluating the efficiency of these scenarios in meeting the objectives of the "expiration time" field, we can identify the most effective method for ensuring proper resource management.

The first approach, "Selective Query Filtering," has been chosen as the primary focus of this study due to its potential for minimizing overhead while maintaining data integrity. The comparative analysis of the three scenarios will provide valuable insights into the impact of different handling techniques for the "expiration time" field on system performance using codebase version 1.1. This assessment will help identify potential optimizations and guide the selection of the most suitable approach for managing resources with an expiration time, ultimately enhancing the overall efficiency of the system.

To assess the performance of the three scenarios in the test case, we have designed a simple algorithm that sends a total of 500 requests, each retrieving data through SELECT queries to the target route for each scenario, considering the five parameters mentioned above. This algorithm measures the time taken for each request in seconds and accumulates the minimum, maximum, and total time. Additionally, it calculates the squared difference of each request's time and the average time to determine the standard deviation. By using this algorithm, we can consistently evaluate the time efficiency of the "Selective Query Filtering" approach in comparison to the other scenarios. The pseudocode for this algorithm is shown in Algorithm \ref{alg:time_measurement}.

\begin{algorithm}
\caption{Pseudocode for Time Measurement}\label{alg:time_measurement}
\begin{algorithmic}[1]
\State Initialize $\textit{count}$, $\textit{sum}$, $\textit{min}$, $\textit{max}$, $\textit{sq\_diff\_sum} \gets 0$
\State $\textit{min} \gets$ large value
\For{$i \gets 1$ to $500$}
    \State Send a request to the target route
    \State Measure the time taken for the request ($\textit{time\_ms}$)
    \If{$\textit{time\_ms} < \textit{min}$}
        \State $\textit{min} \gets \textit{time\_ms}$
    \EndIf
    \If{$\textit{time\_ms} > \textit{max}$}
        \State $\textit{max} \gets \textit{time\_ms}$
    \EndIf
    \State $\textit{sum} \gets \textit{sum} + \textit{time\_ms}$
\EndFor
\State Calculate $\textit{average} \gets \frac{\textit{sum}}{500}$
\For{$i \gets 1$ to $500$}
    \State $\textit{diff} \gets \textit{time taken for request i} - \textit{average}$
    \State $\textit{sq\_diff} \gets \textit{diff} \times \textit{diff}$
    \State $\textit{sq\_diff\_sum} \gets \textit{sq\_diff\_sum} + \textit{sq\_diff}$
\EndFor
\State Calculate $\textit{standard\_deviation} \gets \sqrt{\frac{\textit{sq\_diff\_sum}}{500}}$
\end{algorithmic}
\end{algorithm}

With the algorithm in place, we can proceed to the actual testing phase. For each of the three scenarios, we will run the algorithm on "Machine 1" and record the minimum, maximum, average, and standard deviation of the request times. This data will allow us to thoroughly analyze the performance impact of each approach and draw conclusions about their efficiency in meeting the "expiration time" field objectives. The results of the tests are presented in Table \ref{tab:performance_comparison}.

\begin{table*}[htbp]
\centering
\caption{Performance comparison of the three scenarios}
\begin{tabular}{lcccc}
\hline
Scenario & Min (s) & Max (s) & Avg (s) & Std Dev (s) \\ \hline
Original Codebase without Expiration Time Condition & 0.614603 & 0.683479 & 0.626082 & 0.007875 \\
Non-Indexed Expiration Time Condition & 0.614021 & 0.672247 & 0.622916 & 0.007017 \\
Indexed Expiration Time Condition & 0.629939 & 1.446011 & 0.640483 & 0.045949 \\ \hline
\end{tabular}
\label{tab:performance_comparison}
\end{table*}

Upon examining the results in Table \ref{tab:performance_comparison}, we can derive several insights into the efficiency of the "Selective Query Filtering" approach in managing resources with an expiration time. These insights will be valuable in guiding future optimizations and ensuring proper resource management within the system.

It is important to highlight the trade-offs when comparing the three scenarios. First, the original codebase without the expiration time condition exhibits slightly better average performance compared to the indexed expiration time condition scenario. This result is expected, as the original codebase does not include any additional checks for expired resources, which results in reduced complexity and faster query times. However, the difference in average time between the original codebase and the non-indexed expiration time condition scenario is minimal, indicating that the added complexity of checking for expiration does not significantly impact the overall performance.

When comparing the non-indexed expiration time condition scenario with the indexed expiration time condition scenario, we observe a slight increase in the average time for the indexed case, which might seem counterintuitive as indexing usually improves query performance. However, it is essential to consider that the standard deviation for the indexed expiration time condition scenario is considerably higher, indicating more varied response times. This variability could be attributed to the database's specific structure, the distribution of data within the tables, or the types of queries executed during the test. Despite the higher average time, the indexed expiration time condition scenario offers a more efficient approach to managing resources with an expiration time. The increased standard deviation highlights the variability in response times, which can be attributed to various factors such as the database's structure or the nature of the queries executed during the test. Consequently, the indexed scenario demonstrates greater scalability and adaptability in handling larger volumes of data, making it the preferred choice for implementation.

As the system evolves, the indexed expiration time condition scenario will be the one that prevails and will be part of the solution implementation. It is recommended to further investigate the factors contributing to the variability in response times for this scenario and explore the performance impact of different limit values. This analysis will help guide future optimizations and enhance the system's overall efficiency while meeting the expiration time field objectives.

Furthermore, it is essential to note that the "discovery" request route being tested involves five parameters, which add complexity to the evaluation of each scenario's performance. As the limit value of 50 rows listed increases, the query's latency may grow exponentially. Therefore, the current test with a default limit of 50 rows serves as a starting point, and it is crucial to evaluate the performance of these scenarios with varying limit values to understand their impact on the overall system efficiency.

In conclusion, the "Selective Query Filtering" approach has shown promise in terms of meeting the expiration time field objectives with a minimal impact on database query performance. The small performance difference between the original codebase without the expiration time condition and the non-indexed expiration time condition scenario indicates that checking for expiration does not drastically impact performance. It is recommended to investigate the indexed expiration time condition scenario further to understand the variability in its response times and to explore the performance impact of different limit values. This analysis will guide future optimizations and help select the most suitable approach for managing resources with an expiration time, ultimately enhancing the system's overall efficiency.

\subsection{Comparative Analysis of Latency for CRUD Operations Using the Proposed Solution and OpenMTC}

In this section, we delve into a detailed test case that focuses on the comparative analysis of latency for CRUD (Create, Read, Update, and Delete) operations using the proposed solution and OpenMTC. The objective of this test case is to evaluate the performance of our solution in managing resources against the established OpenMTC standard, providing a benchmark for assessing the efficiency and effectiveness of our approach.

The CRUD operations will be performed through REST requests over the network, rather than on a local machine, as the primary goal of the oneM2M gateway is to facilitate the exchange of information across a distributed network infrastructure. This setup simulates real-world conditions where gateway systems must efficiently handle remote requests, thereby providing a more accurate representation of the solution's performance in practical applications.

To carry out this test case, we will use "Machine 2", as shown in Table \ref{tab:machinesSpecs} as described in the previous test case, with the following specifications: Apple M1 Pro chip, 10 cores, 16 GB RAM, and macOS version 12.6. The codebase version for the proposed solution is 1.2. The test will be conducted using a shell script that sends 500 Application Entity (AE) resource CRUD operations for both the proposed solution and the OpenMTC platform.

The shell script operates as follows: It begins by initializing variables and CSV files for each CRUD operation (POST, PUT, GET, and DELETE). Then, it iterates 500 times, performing the following steps in each iteration: sending a POST, PUT, GET, and DELETE request to the respective servers, measuring the time taken for each request, and appending the time to the corresponding CSV file. After completing the 500 iterations, the script calculates the minimum, maximum, average, and standard deviation of the times for each CRUD operation from their respective CSV files. The results of these calculations provide insights into the latency of the proposed solution and OpenMTC for each CRUD operation.

By comparing the latency for 500 Application Entity (AE) resource CRUD operations on both platforms, we can highlight the strengths and areas for improvement of our proposed solution. This analysis will not only help us to understand the impact of our solution on system performance but also to identify any potential bottlenecks and areas that could benefit from optimization, particularly in the context of network latency and resource management.

The insights gained from this test case will be crucial in the ongoing development and refinement of our solution, ultimately contributing to a more robust and efficient system for managing resources in the context of the oneM2M standard while accounting for the network-related challenges inherent in distributed systems.

\subsection{Cross-Platform Evaluation: Installation and Compilation on Ubuntu and macOS Systems}

In this section, we introduce a test case aimed at evaluating the cross-platform compatibility of our proposed solution through installation and compilation on two widely-used operating systems, Ubuntu and macOS. The objective of this test case is to ensure that our solution can be seamlessly integrated into a variety of operating environments, thereby demonstrating its versatility and adaptability to different system configurations.

A cross-platform evaluation is crucial in the realm of computer science, as it verifies the portability and interoperability of software across different platforms. By validating our solution's compatibility with both Ubuntu and macOS, we demonstrate its ability to function effectively in diverse contexts, catering to a broader range of users and potential applications.

This test case will involve the installation of necessary dependencies, compilation of the source code, and execution of the proposed solution on both operating systems. The evaluation will not only focus on the successful completion of these tasks but also on identifying any platform-specific issues, discrepancies, or bottlenecks that may arise during the process.

The insights gained from this cross-platform evaluation will contribute to the ongoing refinement of our solution, ensuring that it remains robust and effective across various operating environments. This, in turn, enhances its potential for widespread adoption and application in the context of the oneM2M standard.

In this test case, we successfully installed and compiled the proposed solution on Ubuntu, one of the two operating systems being evaluated for cross-platform compatibility. The process began by importing the source code to "Machine 2", as shown in Table \ref{tab:machinesSpecs}, using SFTP, followed by the installation of the necessary dependency, SQLite3, via the Advanced Package Tool (APT).

The proposed solution was successfully compiled using the provided Makefile, which facilitated the creation of the executable without any issues. A closer examination of the dependencies of the compiled executable was carried out using the 'ldd' command. The command revealed that the proposed solution relies exclusively on system libraries such as libdl.so.2, libpthread.so.0, and libc.so.6, which provide essential functionality for dynamic loading, multi-threading, and standard C functions, respectively.

By relying solely on system libraries, the proposed solution demonstrates a significant advantage in terms of portability and maintainability. This approach eliminates the need for third-party dependencies, reducing potential compatibility issues and simplifying the installation process across various platforms. Furthermore, it ensures that the solution benefits from the stability and performance optimizations provided by the operating system's native libraries, which are generally well-maintained and frequently updated.

In summary, the use of system libraries for the proposed solution highlights its robustness and adaptability to different system configurations, ultimately enhancing its potential for widespread adoption and application in the context of the oneM2M standard.

The successful installation, compilation, and identification of dependencies on Ubuntu demonstrate that the proposed solution is adaptable to various operating environments. This adaptability is essential in the realm of computer science as it ensures the portability and interoperability of the software across different platforms. By validating our solution's compatibility with Ubuntu, we have shown its ability to cater to a broader range of users and potential applications.

As the next step in the cross-platform evaluation, the proposed solution will be installed and compiled on macOS. This process will involve identifying and installing any necessary dependencies, compiling the source code, and executing the solution on this operating system. Similar to the evaluation on Ubuntu, the focus will be on the successful completion of these tasks and identifying any platform-specific issues, discrepancies, or bottlenecks that may arise.

In this test case, the proposed solution was successfully installed and compiled on the macOS operating system. The source code was imported to "Machine 3", and the necessary dependency, SQLite3, was found to be already installed on the system. The provided Makefile facilitated the compilation of the files, resulting in the generation of the executable without any issues.

The examination of the dependencies of the compiled executable revealed that the proposed solution relies exclusively on system libraries. The 'ldd' command showed that the solution depends on /usr/lib/libSystem.B.dylib, which provides essential functionality for dynamic loading, memory allocation, and system calls. By relying solely on system libraries, the proposed solution demonstrates a significant advantage in terms of portability and maintainability, as it eliminates the need for third-party dependencies.

The use of system libraries highlights the proposed solution's adaptability to different system configurations, ultimately enhancing its potential for widespread adoption and application in the context of the oneM2M standard. This approach ensures that the solution benefits from the stability and performance optimizations provided by the operating system's native libraries, which are generally well-maintained and frequently updated.

By successfully installing and compiling the proposed solution on both Ubuntu and macOS, we have demonstrated its ability to function effectively in diverse contexts, catering to a broader range of users and potential applications. The insights gained from this cross-platform evaluation will inform any necessary adjustments or refinements to ensure seamless integration into various operating environments.

In conclusion, the cross-platform evaluation provides a comprehensive understanding of the proposed solution's compatibility with different operating systems, demonstrating its adaptability and versatility to various system configurations. The use of system libraries ensures the solution's portability and maintainability, while also enhancing its potential for widespread adoption and application in the context of the oneM2M standard.

\subsection{Comparative Analysis of Latency for Resource Discovery Requests Using the Proposed Solution and OpenMTC}

In this section, we present a fourth test case aimed at evaluating the relative efficiency of our proposed solution compared to OpenMTC in the context of resource discovery requests. This test case is essential as it enables us to gauge the effectiveness of our approach in terms of discovering resources in a large-scale environment, thereby providing insights into the scalability and performance of our solution.

To conduct this test case, we will be using "Machine 2", as it is the most suitable machine for testing. We will divide the test into three different scenarios, each with varying numbers of resources in the database (500, 1000, and 2000). By comparing the latency of resource discovery requests for each scenario using the proposed solution and OpenMTC, we can determine which solution performs better in a large-scale environment.

Resource discovery is a crucial aspect of the oneM2M standard, enabling clients to search for and access resources within the system. As such, the latency of resource discovery requests is a significant factor in determining the overall performance and effectiveness of an oneM2M-compliant system. By conducting this test case, we can identify potential bottlenecks or areas for improvement in our proposed solution, enhancing its scalability and performance in the context of large-scale resource discovery requests.

The insights gained from this test case will contribute to the ongoing refinement and optimization of our solution, ensuring that it remains effective and competitive in the realm of oneM2M-compliant systems.




\subsection{Minimal Hardware Compatibility Testing: ESP32 Case Study}

In this section, we present a test case focused on assessing the compatibility of our proposed solution with minimal hardware, specifically using the ESP32 microcontroller as a case study. The objective of this test case is to verify the suitability of our solution for resource-constrained environments, thereby broadening its potential applications and enabling its adoption in a wider range of use cases.

Minimal hardware compatibility is an essential aspect of computer science, as many real-world applications require efficient and lightweight solutions that can run on devices with limited processing power, memory, and energy resources. By evaluating the performance of our proposed solution on the ESP32, a popular and cost-effective microcontroller with integrated Wi-Fi and Bluetooth capabilities, we can demonstrate its ability to function effectively under such constraints.

This test case will involve the adaptation, deployment, and execution of our solution on the ESP32 platform. The evaluation will not only focus on the successful completion of these tasks but also on identifying any hardware-specific issues, discrepancies, or bottlenecks that may arise during the process. Additionally, we will assess the resource usage, including memory and processing overhead, to ensure that the solution remains viable and efficient in resource-constrained scenarios.

The insights gained from this minimal hardware compatibility testing will contribute to the ongoing refinement of our solution, ensuring that it remains both robust and versatile, capable of meeting the demands of a diverse range of applications in the context of the oneM2M standard.






%Avaliação do impacto resultante do cumprimento do objetivo do campo expiration time
%
%Especificações da máquina da escola
%Especificações da rede
%Meter o executável a funcionar na máquina com o ficheiro de configuração
%
%Shell script para executar 500 POST, guardar minimo, maximo, média, desvio padrão
%Shell script para executar 500 PUT, guardar minimo, maximo, média, desvio padrão
%Shell script para executar 500 GET, guardar minimo, maximo, média, desvio padrão
%Shell script para executar 500 DELETE, guardar minimo, maximo, média, desvio padrão
%
%
%Quanto tempo demora a POST/PUT/GET/DELETE 500 AEs usando a nossa solução e a solução disponível do OpenMTC na mesma máquina (na maquina do MEI-CM) na rede - Saber o tempo de cada um dos 500 e calcular, minimo, maximo, média, desvio padrão
%
%Instalação e Compilação em máquinas Ubuntu e MacOS (verificar se corre em outros SOs)
%
%? Fazer testes com Hardware minimo - por exemplo ESP32 ?

\section{Conclusion}
\label{conclusion}
\printcredits

%% Loading bibliography style file
%\bibliographystyle{model1-num-names}
%\bibliographystyle{cas-model2-names}
\bibliographystyle{unsrt} % Estilo de Bibliografia
% Loading bibliography database
\bibliography{cas-refs}


%\vskip3pt

\bio{figs/bioFig}
My name is Rafael Pereira, I'm 22 years old, and I'm an MSc Computer Engineering Student and Researcher at Polytechnic of Leiria. I'm extremely curious about the technology world, ambitious for knowledge in this area, I'm dedicated to doing my work. The programming thing always got me, and every day it grows. Today I'm looking for interesting projects, that can make me think and learn every day.
\endbio

\end{document}
