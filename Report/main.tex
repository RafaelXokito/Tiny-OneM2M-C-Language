%% 
%% Copyright 2019-2020 Elsevier Ltd
%% 
%% This file is part of the 'CAS Bundle'.
%% --------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'CAS Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for cas-dc documentclass for 
%% double column output.

%\documentclass[a4paper,fleqn,longmktitle]{cas-dc}
\documentclass[a4paper,fleqn]{cas-dc}

%\usepackage[authoryear,longnamesfirst]{natbib}
%\usepackage[authoryear]{natbib}
\usepackage[numbers]{natbib}

%%%Author definitions
\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}
\tsc{EP}
\tsc{PMS}
\tsc{BEC}
\tsc{DE}
%%%
% -------------------------------------------------------------------
% Pacotes para inserção de figuras e subfiguras
%\usepackage{subfig,epsfig,tikz,float}		            % Packages de figuras. 
\usepackage{graphicx}

\usepackage{caption}
\usepackage{subcaption}

\graphicspath{ {./figs/} }
% -------------------------------------------------------------------
% \usepackage{amssymb}
% -------------------------------------------------------------------
% Pacotes para inserção de tabelas
\usepackage{booktabs,multicol,multirow,tabularx,array}          % Packages para tabela
\usepackage{natbib}
\usepackage{pifont}
\usepackage{xcolor}
\usepackage{algpseudocode}
\usepackage{algorithm}
% -------------------------------------------------------------------
\PassOptionsToPackage{style=super,nolist}{glossaries}
\PassOptionsToPackage{acronym}{glossaries}
\PassOptionsToPackage{nonumberlist}{glossaries}
\usepackage{glossaries}
\newacronym{ai}{AI}{Artificial Intelligence}
\newacronym{iot}{IoT}{Internet of Things}
\newacronym{iomt}{IoMT}{Internet of Medical Things}
\newacronym{mtc}{MTC}{Machine Type Communication}
\newacronym{m2m}{M2M}{Machine To Machine}
\newacronym{tls}{TLS}{Transport Layer Security}
\newacronym{dtls}{DTLS}{Datagram Transport Layer Security}
\newacronym{iotdm}{IoTDM}{Internet of Things Device Management}
\newacronym{sdn}{SDN}{Software-Defined Networking}
\newacronym{http}{HTTP}{Hypertext Transfer Protocol}
\newacronym{coap}{CoAP}{Constrained Application Protocol}
\newacronym{mqtt}{MQTT}{Message Queuing Telemetry Transport}
\newacronym{rtt}{RTT}{Round Trip Time}
\newacronym{ap}{AP}{Access Points}
\newacronym{rsus}{RSUs}{Roadside Units}
\newacronym{qos}{QoS}{Quality of Service}
\newacronym{ccas}{CCAS}{Cluster-based Congestion-mitigating Access Scheme}
\newacronym{mtcds}{MTCDs}{Machine-type Communication Devices}
\newacronym{mtcd}{MTCD}{Machine-type Communication Device}
\newacronym{bs}{BS}{Base Station}
\newacronym{mtcg}{MTCG}{Machine-type Communication Gateway}
\newacronym{ca}{CA}{Certificate Authority}
\newacronym{p2p}{P2P}{Peer-to-Peer}
\newacronym{rest}{REST}{Representational state transfer}
\newacronym{api}{API}{Application Programming Interfaces}
\newacronym{crud}{CRUD}{Create, Retrieve, Update, Delete}
\newacronym{ae}{AE}{Application Entity}
\newacronym{aes}{AEs}{Application Entities}
\newacronym{cnt}{CNT}{Container}
\newacronym{cnts}{CNTs}{Containers}
\newacronym{cin}{CIN}{Container Instance}
\newacronym{cins}{CINs}{Container Instances}
\newacronym{sub}{SUB}{Subscription}
\newacronym{subs}{SUBs}{Subscriptions}
\newacronym{etsi}{ETSI}{European Telecommunications Standards Institute}
\newacronym{pi}{PI}{Parent Identifier}
\newacronym{cse}{CSE}{Common Services Entity}
\newacronym{ram}{RAM}{Random Access Memory}
\newacronym{json}{JSON}{JavaScript Object Notation}
\makeglossaries
% -------------------------------------------------------------------
\usepackage[utf8]{inputenc} % The default since 2018
\DeclareUnicodeCharacter{200B}{{\hskip 0pt}}
% -------------------------------------------------------------------
\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}
\shorttitle{Leveraging social media news}
\shortauthors{CV Radhakrishnan et~al.}

\title [mode = title]{Tiny oneM2M C Language API}                     

\credit{Conceptualization of this study, Methodology, Software}

\author[1]{Rafael Pereira}[type=editor,
                        %auid=000,bioid=1,
                        linkedin=rafaelmendespereira,
                        orcid=0000-0001-8313-7253]
%\cormark[1]
%\fnmark[1]
\ead{rafael.m.pereira@ipleiria.pt}
   
\address[1]{Computer Science and Communications Research Centre, School of Technology and Management, Polytechnic of Leiria, 2411-901 Leiria, Portugal}

\begin{abstract}
This template helps you to create a properly formatted \LaTeX\ manuscript.

\noindent\texttt{\textbackslash begin{abstract}} \dots 
\texttt{\textbackslash end{abstract}} and
\verb+\begin{keyword}+ \verb+...+ \verb+\end{keyword}+ 
which
contain the abstract and keywords respectively. 

\noindent Each keyword shall be separated by a \verb+\sep+ command.
\end{abstract}

%\begin{graphicalabstract}
%\includegraphics{figs/grabs.pdf}
%\end{graphicalabstract}
%
%\begin{highlights}
%\item Research highlights item 1
%\item Research highlights item 2
%\item Research highlights item 3
%\end{highlights}

\begin{keywords}
quadrupole exciton \sep polariton \sep \WGM \sep \BEC
\end{keywords}


\maketitle

\section{Introdution}

The \gls{iot} encompasses a vast network of diverse entities, including tags, sensors, embedded devices, handheld devices, and backend servers. It enables the development of novel services and applications across various domains such as civil transportation, electric power grids, medical treatment, and industrial and home automation \cite{kim_m2m_2014}. 

\gls{m2m} denotes technologies that enable communication and networking of different devices and is a part of the field of \gls{iot}. While the \gls{iot} focuses on the interconnection of physical objects and their interaction with humans, the \gls{m2m} system focus towards seamless connectivity. It involves automated systems where devices autonomously gather data (e.g., temperature, humidity, speed, position, heartbeat) from remote sources, exchange information, and execute actions based on control messages. These operations generally occur without or with minimal human intervention, utilizing public network infrastructures. Consequently, the \gls{m2m} system plays a crucial role in realizing the full potential of the \gls{iot} \cite{kim_m2m_2014}, \cite{lawton_machine_2004}, \cite{pticek_architecture_2015}.

\gls{m2m} systems are typically characterized by their heterogeneity, lack of structure, and loose integration, achieved through standardized communication technologies. These technologies include WiFi, Zigbee, Ethernet, and power line communications, among others. By leveraging standardized technologies, \gls{m2m} systems enable seamless interoperability among devices and simplify mass-produced, standards-compliant equipment. This approach contributes to cost-effective implementations, reducing expenses associated with system development and deployment \cite{lawton_machine_2004}, \cite{cao_survey_2016}.

The rapid expansion of \gls{m2m} technologies has compelled various standardization bodies, such as the  \gls{etsi}, to establish \gls{m2m} standards, resulting in the emergence of multiple standards (e.g \gls{etsi} \gls{m2m} and OneM2M). Consequently, the OneM2M initiative was established to develop a unified global specification and standard for \gls{m2m} communication \cite{pticek_architecture_2015}.

In 2008, \gls{etsi} initiated the standardization process for \gls{m2m} by examining requirements and identifying gaps in existing standards. \gls{etsi} began defining an \gls{m2m} platform that could cater to various sectors such as smart metering, e-health, transportation, manufacturing, facility management, and more. In February 2015, OneM2M introduced its first global standard. OneM2M shared the same objective as \gls{etsi} \gls{m2m} but aimed to achieve it on a global level \cite{pticek_architecture_2015}. 

Therefore, the oneM2M \footnote{OneM2M official page: \url{https://www.onem2m.org/}} standard aims to establish uniformity in the communication between devices, servers, and applications by offering a standardized \gls{m2m} service. It defines a platform centered around the concept of resources and \gls{rest} \gls{api}. Moreover, it encompasses over a dozen Common Service Functions such as registration, subscription, discovery, and data management, among others \cite{onem2m_standard}.

In this paper, we present a novel implementation of the OneM2M standard using a low-level programming language to address the limitations of high-level language-based implementations. Our approach is designed to deliver superior performance, reduced memory overhead, and increased control over hardware resources. By focusing on key challenges associated with \gls{mtc} gateways, such as protocol interoperability, scalability, and latency, our implementation aims to provide a robust and efficient solution for the evolving \gls{iot} landscape.

\textcolor{red}{Paper structure} The remainder of this paper is organized as follows: Section \ref{relatedWork} provides a comprehensive review of the related work in \gls{m2m} implementations, discussing various programming languages and architectural choices, as well as the importance of interoperability in \gls{iot} systems. Section \ref{architecture} delves into the system architecture and the specific modules that constitute our TinyOneM2M software. In Section \ref{sec:tinyonem2msolution}, we provide a comprehensive overview of our implementation of the OneM2M version 5.1 protocol, detailing the system components that constitute our solution. The testing mechanisms employed to examine the TinyOneM2M solution under various scenarios were detailed in Section \ref{tests}. Section \ref{sec:lessonsLearned} encompasses the acquired knowledge and reflective insights gained throughout the system's development. Finally, Section \ref{conclusion} concludes the paper and highlights potential future research directions.

\section{Related work}
\label{relatedWork}

OneM2M provides a standardized framework and protocols to enable interoperability between diverse \gls{iot} devices. It can be regarded as an overarching framework that facilitates the implementation of \gls{m2m} solutions, allowing for seamless communication and integration of various machine-to-machine interactions within an \gls{iot} ecosystem \cite{onem2m_standard}. Therefore, in this section, we will provide an overview of the current state-of-the-art in \gls{m2m} implementations, highlighting the advantages and disadvantages of various programming languages and architectural choices.

Rubi et al. \cite{Rubi2019} propose an \gls{iomt} platform with a cloud-based electronic health system that collects data from various sources and relays it through gateways or fog servers. The platform enables data management, visualization of electronic records, and knowledge extraction through big data processing, machine learning, and online analytics processing. Additionally, it offers data sharing services for third-party applications, improving healthcare services and interoperability.

The authors in \cite{Volkov2017} conducted \gls{iotdm} service testing for smart city management, focusing on the ecological situation in Saint-Petersburg's central district, using an \gls{sdn} network infrastructure. They developed a hierarchical model and \gls{iot} traffic generator in compliance with oneM2M specifications and implemented \gls{http}, \gls{coap}, and \gls{mqtt} protocols. Their findings suggested that \gls{http} is the most suitable protocol for registering \gls{iot} devices to the \gls{iotdm} service, while \gls{mqtt} is optimal for transmitting sensor data. The authors concluded that the \gls{sdn} approach significantly reduces the \gls{rtt} parameter, demonstrating its potential in managing \gls{iot} traffic in smart city environments.

Soumya et al. \cite{Datta2015} explore Fog Computing as a consumer-centric \gls{iot} service deployment platform, particularly for connected vehicles and intelligent transportation systems requiring low latency, high mobility, real-time data analytics, and wide geographic coverage. They present an \gls{iot} architecture for connected vehicles, integrating Fog Computing platforms into oneM2M standard architecture. The architecture comprises virtual sensing zones, \gls{ap}, \gls{rsus}, \gls{m2m} gateways, and cloud systems. Fog Computing enables consumer-centric services such as \gls{m2m} data analytics with semantics, discovery of \gls{iot} services, and management of connected vehicles. The paper highlights the advantages of Fog Computing, including reduced latency, improved \gls{qos}, real-time data analysis, and actuation for superior user experience and consumer-centric \gls{iot} products.

This paper \cite{Liang2018} presents a \gls{ccas} that addresses the issue of collision and access efficiency in \gls{m2m} communications. The authors propose a modified spectral clustering algorithm to form clusters based on the location and service requirements of \gls{mtcds}, ensuring that devices with similar requirements are grouped together. They also develop an \gls{mtcg} selection algorithm that considers the delay requirements of \gls{mtcds} and selects an \gls{mtcg} with the appropriate forwarding threshold. The proposed CCAS divides the data transmission process into an \gls{mtcd}-\gls{mtcg} stage and an \gls{mtcg}-\gls{bs} stage to reduce collision probability and increase the number of successfully received packets, without causing additional average access delays for \gls{mtcds}. The paper includes a theoretical analysis of the \gls{mtcg} aggregation and forwarding process, as well as performance evaluations through simulations.

The paper \cite{Thielemans2019} proposes a resource-efficient secure implementation of OneM2M based on C programming language for \gls{iot}, while also performing an analysis of the OneM2M architecture and protocols. Furthermore, the authors highlight the relevance of a lightweight implementation using optimized data structures and algorithms for \gls{iot} devices with limited resources and propose a solution that can reduce computational costs and processing time. Secure communication protocols such as \gls{tls} and \gls{dtls}, as well as authentication and encryption mechanisms, were used to ensure security. The authors also suggested that the cryptography algorithms chosen must be lightweight to reduce the computational burden on \gls{iot} devices. The evaluation of this implementation meant using a set of benchmarks while comparing the developed OneM2M implementation with existing ones. Results showed that, in terms of memory usage and processing time, the author's proposed implementation of OneM2M using C language is more resource-efficient.

The authors in \cite{Benoygopal2021} present a secure authentication and access control scheme for OneM2M-based \gls{iot} systems. Moreover, propose an approach to address the challenge of identity and access management in large-scale \gls{iot} systems. The proposed scheme is based on the role-based access control model, where system administrators can define access policies based on the roles of the users. Their scheme entails a secure authentication and authorization mechanism based on certificates, which are issued by a trusted \gls{ca}. The evaluation process used a prototype implementation of the OneM2M standard and compared it with existing access control schemes. Therefore concluding that their scheme achieves better performance and scalability while providing a flexible and fine-grained access control mechanism. However, the proposed scheme contains security and privacy issues for which the authors presented several countermeasures to mitigate them.

The paper \cite{Kim2018} discusses the design, implementation and evaluation of an interoperable platform for the Internet of Things \gls{iot} using the OneM2M standard, where the authors focused on implementing the standard using C language. Moreover, the authors discussed how their C implementation handles registration, discovery and subscription-resource management tasks while proposing optimizations to improve performance. Experiments were designed to assess the scalability, reliability, and processing time with different loads and data sizes. Those experiments concluded that the C implementation of OneM2M achieved better performance than other implementations regarding response time and memory usage. Regarding security, the paper also details oneM2M's security features, namely, authentication, authorization, and confidentiality, how the C implementation handles these features and how to improve the platform's security. Interoperability features, such as standardized interfaces and protocols, and their C implementation were also detailed in the paper since one of oneM2M's main objects must be the interoperability between different \gls{iot} platforms and devices.


% https://www.scopus.com/results/results.uri?sort=plf-f&src=s&st1=%28onem2m+OR+mtc%29+and+%22API%22&sid=2e6f61fce202fa9e6635e553544fee3c&sot=b&sdt=b&sl=40&s=TITLE-ABS-KEY%28%28onem2m+OR+mtc%29+and+%22API%22%29&origin=searchbasic&editSaveS=&yearFrom=Before+1960&yearTo=Present

\section{Architecture}
\label{architecture}

The architecture of TinyOneM2M, a compact, cross-platform, and easy-to-use implementation of the OneM2M standard \cite{onem2m_standard}, is meticulously designed to adapt to various scenarios. The flexible architecture supports different modes of operation, allowing it to integrate seamlessly into existing infrastructure. This section details the system architecture and the specific modules that constitute the TinyOneM2M software.

\subsection{System Architecture}

The system architecture of TinyOneM2M is developed considering the diverse range of scenarios that could arise in \gls{iot} settings. Three primary scenarios have been identified where TinyOneM2M can be effectively employed, namely, the Home Gateway Scenario, the Single Devices Scenario (\gls{p2p}), and the Cloud Scenario.

\begin{figure}[htb]
     \centering
     \begin{subfigure}[h]{7cm}
         \centering
         \includegraphics[width=6cm]{HomeGatewaysScenario.jpg}
         \caption{Home Gateway Scenario.}
         \label{fig:hgw}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{7cm}
         \centering
         \includegraphics[width=6cm]{SingleDevicesScenario.jpg}
         \caption{Single Devices Scenario (\gls{p2p}).}
         \label{fig:sdc}
     \end{subfigure}
     \hfill
     \begin{subfigure}[h]{7cm}
         \centering
         \includegraphics[width=6cm]{CloudScenario.jpg}
         \caption{Cloud Scenario.}
         \label{fig:cs}
     \end{subfigure}
     \caption{TinyOneM2M System Architecture.}
     \label{fig:scenarios}
\end{figure}

In the Home Gateway Scenario (Figure \ref{fig:hgw}), TinyOneM2M serves as a central gateway within a home network, facilitating communication between different devices in the network. This centralized approach allows for efficient data transfer and coordination between devices.

The Single Devices Scenario (Figure \ref{fig:sdc}) represents a decentralized, \gls{p2p} approach where each device contains its instance of TinyOneM2M. This approach allows devices to communicate directly with each other, enhancing scalability and data transfer efficiency by eliminating the need for a central point of communication.

In the Cloud Scenario (Figure \ref{fig:cs}), TinyOneM2M is hosted on a cloud provider and can be distributed in several machines/instances and be managed by a load balancer, allowing devices to connect and communicate via the internet. This scenario facilitates large-scale, distributed \gls{iot} applications, providing the advantages of cloud computing such as high availability, scalability, and efficient data management.

\subsection{TinyOneM2M Architecture}\label{tinyonem2marchitecture}

TinyOneM2M is architected in a way that distinctly separates responsibilities across various modules, providing a robust and efficient system. Its architecture is underpinned by a seamless flow of operations that handles and processes incoming \gls{http} \gls{rest} requests, ultimately leading to action being taken, such as data persistence or triggering notifications. An overview of the architecture is depicted in Figure \ref{fig:TinyOneM2Mchitecture} and will be described in the following sections dividing each module by section.

\begin{figure*}[htbp]
	\centering
	\includegraphics[width=\linewidth]{SolutionArchitecture.jpg}
	\caption{TinyOneM2M System Architecture.}
	\label{fig:TinyOneM2Mchitecture}
\end{figure*}

\subsubsection{Overview of Resource Oriented Architecture}

A crucial aspect of TinyOneM2M's system architecture, as depicted in Figure \ref{fig:systemarchitecture}, is the interaction between third-party applications and TinyOneM2M, and between TinyOneM2M and external \gls{mqtt} brokers or \gls{http} web servers. This figure outlines these interactions and the hierarchy of resources (described in Section \ref{sec:M2MResource}) within the TinyOneM2M system.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{SystemArchitectuRE.jpg}
	\caption{System Architecture.}
	\label{fig:systemarchitecture}
\end{figure}

Third-party applications communicate with TinyOneM2M to create or retrieve resources. They send \gls{http} \gls{rest} requests to the \gls{rest} Webservice module in TinyOneM2M, which acts as an intermediary, accepting and processing various types of requests, including POST, GET, PUT, and DELETE. Depending on the type of request, the application can create a new resource, retrieve details of an existing resource, update a resource, or remove a resource.

TinyOneM2M also communicates with \gls{mqtt} brokers or \gls{http} WebServers, primarily for notifying about events based on subscriptions inserted into the TinyOneM2M system. If a resource is marked as notifiable changes - for instance, if a new data point is added to a monitored sensor - TinyOneM2M's Notification module creates an \gls{mqtt} or \gls{http} client to send a notification to the relevant \gls{mqtt} broker or \gls{http} server at the moment the changes happen. This notification mechanism allows applications to react promptly to changes in monitored resources, making it an integral part of many \gls{iot} scenarios, from anomaly detection in industrial settings to updates in smart home environments.

\subsubsection{WebService REST}\label{sec:archwebservice}

The \gls{rest} Webservice module in TinyOneM2M functions as the system's primary interface, accepting and processing various types of \gls{http} \gls{rest} requests. When a request is received, this module generates a new thread to manage the request, which is then passed onto the next layer for further processing.

The types of \gls{http} request that are supported by TinyOneM2M are primarily structured around \gls{crud} operations for various resources. These operations pertain to different resources identified by OneM2M standards, including \gls{ae}, \gls{cnt}, \gls{cin}, \gls{sub} resources.

For \gls{aes}, users can perform different actions on \glspl{ae} using specific HTTP requests. They can create a new \gls{ae} by sending a POST request, retrieve the details of an \gls{ae} using a GET request, update an \gls{ae} using a PUT request, and remove an \gls{ae} by issuing a DELETE request. These requests are sent to a specific URL structure, targeting the CSE\_Base and the \texttt{AE\_ResourceName}.

For \gls{cins}, the operations mirror those for \gls{aes} but are extended with an additional URL parameter to specify the \gls{cin}. These operations thus allow users to create, retrieve, and delete \gls{cins} within a specific \gls{ae} and \gls{cnt} resource.

Similarly, \gls{sub} resources can be managed with POST, GET, PUT, and DELETE requests. These requests target a specific \texttt{SUB\_ResourceName} within a specified CSE\_Base, \texttt{AE\_ResourceName}, and \texttt{CNT\_ResourceName}.

The system also provides functionality to retrieve the root resource information with a specific GET request and perform discovery operations. The discovery operation uses a GET request with query parameters to filter resources based on their type and labels. These operations can be used to retrieve general or specific information about the resources available in the system.

\subsubsection{Response Processor} \label{sec:archresponseprocessor}

The instantiated thread uses the Response Processor module to validate the request, determine the HTTP method, and conduct preliminary validations. The processed request is then forwarded to the next module for further action.

\subsubsection{M2M Protocol Wrapper}

The \gls{m2m} Protocol Wrapper is responsible for processing and validating the incoming request according to the OneM2M standard. It acts as a bridge between the protocol-specific details of the request and the \gls{m2m} resource module, ensuring that all processed requests comply with the OneM2M standard.

\subsubsection{M2M Resources}\label{sec:M2MResource}

The \gls{m2m} Resources module in TinyOneM2M is a crucial component that handles data persistence and retrieval operations. Acting as an intermediary between the incoming requests and the database, this module either persists data to the database or retrieves data from it, based on the action specified in the processed request. Additionally, it checks if the parent resource associated with the incoming request is notifiable. If it is, the module triggers a new thread for the Notification module, thus effectively bridging the communication between the client and the notification mechanisms.

This module supports different types of resources, following a hierarchical structure as prescribed by the OneM2M standard and as shown in Figure \ref{fig:hierarchy}. The CSE\_Base \gls{cse} resource sits at the root of this hierarchy, providing an entry point for all other resources in the system.

The \gls{ae} is an application-level entity that provides services to end-users. It can encapsulate \gls{cnt} and \gls{sub} resources, which provide data organization and condition-based notification services, respectively.

The \gls{cnt} resource represents a container for organizing data within \gls{ae} and other \gls{cnt} resources. It can contain other \gls{cnt}, \gls{cin}, and \gls{sub} resources, allowing an organized and hierarchical data structure.

The \gls{cin} resource, residing under a \gls{cnt}, encapsulates the actual data or content in the system. The \gls{sub} resource, which can be nested under CSE\_Base, \gls{ae}, and \gls{cnt} resources, is responsible for managing subscriptions. This enables applications to receive notifications about changes in resources they're interested in.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=7cm]{Hierarchy}
	\caption{TinyOneM2M supported resources hierarchy.}
	\label{fig:hierarchy}
\end{figure}

\subsubsection{Embedded Single File Database}

The TinyOneM2M system relies on an embedded single-file database for optimized data storage and retrieval. This format offers high efficiency due to its low overhead and quick access time. The system employs a denormalized structure in certain areas, an approach commonly used in database design to improve performance, even at the expense of data redundancy.

In this setup, there is one main table named \texttt{mtc}. This table in \ref{tab:database} is a combination of multiple entity types in the TinyOneM2M hierarchy, such as \gls{ae}, \gls{cin} and \gls{sub}. The attributes of these entities are represented as columns in the \texttt{mtc} table using their abbreviations, .

Looking deeper into the table structure, the \texttt{ri} (resource identifier) column serves as the primary key, providing unique identification for each record. It establishes relations with the \gls{pi} column, which refers to the \texttt{ri} of the parent record. This sets up a hierarchical relationship between records, which mirrors the resource hierarchy in the TinyOneM2M system.

The table also includes specific columns for different resource attributes. For example, \texttt{ty}, \texttt{rn}, and \texttt{et}. It also stores specific attributes for \gls{ae} like \texttt{aei}, for \gls{sub} like \texttt{nu} and \texttt{enc}, and for \gls{cin} like \texttt{con} and \texttt{cnf}.

Some columns like \texttt{cbs} (current byte size) and \texttt{mni} (maximum byte size) are used for handling quotas and limits in the TinyOneM2M system. For instance, \texttt{cbs} and \texttt{mbs} are related to the storage size occupied by \gls{cin} resource inside a \gls{cnt} and its maximum allowed size, respectively.

The \texttt{mtc} table also includes columns like \texttt{url}, \texttt{lbl}, \texttt{acpi}, \texttt{daci}, and \texttt{poa} (point of access), which store various types of information required for OneM2M standard operations.

To optimize read operations and streamline data retrieval, we have introduced a blob column that stores the \gls{json} object associated with each resource. This approach eliminates the need to select all the columns from the table and map them to the \gls{json} object, as we can now retrieve the complete \gls{json} object directly from the blob column. By doing so, we significantly improve performance and simplify the data retrieval process. Additionally, the database employs indexes on selected columns which allows faster data retrieval and processing, a key requirement for real-time operations in \gls{iot} environments. For instance, the \texttt{sqlite\_autoindex\_mtc\_1} index on the \texttt{ri} column helps in quickly locating records based on their unique identifiers.

\begin{table}[h]
	\scriptsize
	\caption{Atributes designations.}
	\label{tab:attributesDesignations}
	\begin{tabular}{ |>{\bfseries}p{1.5cm}|c| }
		\hline
		\textbf{Attribute Abbreviation} & \textbf{Attribute Designation} \\
		\hline
		\texttt{ty} & Resource type \\
		\hline
		\texttt{ri} & Resource identifier \\
		\hline
		\texttt{rn} & Resource name \\
		\hline
		\texttt{pi} & Parent identifier \\
		\hline
		\texttt{aei} & \gls{ae} identifier \\
		\hline
		\texttt{csi} & \gls{cse} identifier \\
		\hline
		\texttt{cst} & \gls{cse} type \\
		\hline
		\texttt{api} & App identifier \\
		\hline
		\texttt{rr} & Request reachability \\
		\hline
		\texttt{et} & Expiration time \\
		\hline
		\texttt{ct} & Creation time \\
		\hline
		\texttt{lt} & Last modified time \\
		\hline
		\texttt{url} & Url \\
		\hline
		\texttt{lbl} & Labels \\
		\hline
		\texttt{acpi} & Access control policy IDs \\
		\hline
		\texttt{daci} & Dynamic authorization consultation IDs \\
		\hline
		\texttt{poa} & Point of access \\
		\hline
		\texttt{srt} & Serialization time \\
		\hline
		\texttt{blob} & Blob \\
		\hline
		\texttt{cbs} & Current byte size \\
		\hline
		\texttt{cni} & Current number of instances \\
		\hline
		\texttt{mbs} & Maximum byte size \\
		\hline
		\texttt{mni} & Current number of instances \\
		\hline
		\texttt{st} & State tag \\
		\hline
		\texttt{cnf} & Content info \\
		\hline
		\texttt{cs} & Content size \\
		\hline
		\texttt{con} & Content \\
		\hline
		\texttt{nu} & Notification URI \\
		\hline
		\texttt{enc} & Event notification criteria \\
		\hline
	\end{tabular}
\end{table}

\begin{table}[h]
\scriptsize
\caption{Database structure.}
\label{tab:database}
\begin{tabular}{ |>{\bfseries}p{1.5cm}|c|p{3cm}| }
    \hline
    \textbf{Column Name} & \textbf{Type} & \textbf{Reference} \\
    \hline
    \texttt{ty} & INTEGER & - \\
    \hline
    \texttt{ri} & TEXT & PRIMARY KEY \\
    \hline
    \texttt{rn} & TEXT & - \\
    \hline
    \texttt{pi} & TEXT & REFERENCES mtc(ri) ON DELETE CASCADE \\
    \hline
    \texttt{aei} & TEXT & - \\
    \hline
    \texttt{csi} & TEXT & - \\
    \hline
    \texttt{cst} & INTEGER & - \\
    \hline
    \texttt{api} & TEXT & - \\
    \hline
    \texttt{rr} & TEXT & - \\
    \hline
    \texttt{et} & DATETIME & - \\
    \hline
    \texttt{ct} & DATETIME & - \\
    \hline
    \texttt{lt} & DATETIME & - \\
    \hline
    \texttt{url} & TEXT & - \\
    \hline
    \texttt{lbl} & TEXT & - \\
    \hline
    \texttt{acpi} & TEXT & - \\
    \hline
    \texttt{daci} & TEXT & - \\
    \hline
    \texttt{poa} & TEXT & - \\
    \hline
    \texttt{srt} & TEXT & - \\
    \hline
    \texttt{blob} & TEXT & - \\
    \hline
    \texttt{cbs} & INTEGER & - \\
    \hline
    \texttt{cni} & INTEGER & - \\
    \hline
    \texttt{mbs} & INTEGER & - \\
    \hline
    \texttt{mni} & INTEGER & - \\
    \hline
    \texttt{st} & INTEGER & - \\
    \hline
    \texttt{cnf} & TEXT & - \\
    \hline
    \texttt{cs} & INTEGER & - \\
    \hline
    \texttt{con} & TEXT & - \\
    \hline
    \texttt{nu} & TEXT & - \\
    \hline
    \texttt{enc} & TEXT & - \\
    \hline
\end{tabular}
\end{table}

\subsubsection{Notifications}

The Notification module is responsible for sending notifications based on subscription resources. When a resource (e.g. Container) contains a subscription directly inside and other resource (e.g. Content Instance) is created directly inside the resource (e.g. Container) an \gls{mqtt} or \gls{http} client is asynchronous created to send a notification to the respective third-party \gls{mqtt} or \gls{http} broker.

\section{Implementation of TinyOneM2M Solution} \label{sec:tinyonem2msolution}

Our solution adheres to the OneM2M standard version 5.1 \footnote{\url{https://www.onem2m.org/technical/published-specifications\#listofreleases}}, implementing a set of key resources integral to our intended functionalities. We opted for the implementation of the resources CSE\_Base, \gls{ae}, \gls{cnt}, \gls{cin}, and \gls{sub}. The implementation of TinyOneM2M is designed with a strong focus on the system architecture presented in Figure \ref{fig:TinyOneM2Mchitecture} in section \ref{architecture}. Each component of the architecture has a corresponding implementation detail which we aim to elucidate throughout this section.

The development of the TinyOneM2M solution was carried out using the C language. There are several reasons for this choice. Firstly, C is highly portable and efficient, making it ideal for \gls{iot} applications where devices can have varied processing capabilities. It also offers low-level access to memory, which allows fine-tuned control over system resources. However, C also has some disadvantages. It lacks the built-in object-oriented features found in languages like C++ or Python, which can lead to more complex code structures for large systems. Moreover, the C language places the burden of memory management on the developer, which, if not executed with caution, can give rise to potential issues and challenges.

To gauge our level of compliance with the OneM2M standard, we quantified the degree of our implementation based on each resource. We calculated the percentage of implementation by comparing the number of implemented attributes to the total number of attributes suggested by the standard, all multiplied by one hundred.

Based on this calculation, we estimate that our solution covers approximately 57\% of the standard on average. A detailed breakdown of this calculation, per resource, will be further elucidated in the section \ref{sec:m2mresources}.

Our implementation is designed to ensure compatibility with services utilizing the OpenMTC \footnote{\url{https://github.com/OpenMTC/OpenMTC}} SDK, an open-source framework that provides an OneM2M-based version 1.1 platform. This interoperability is possible because the \gls{rest} requests in our implementation have the same structure as those in the OpenMTC implementation and return similar error codes.

Throughout the development process, we've leveraged a variety of external and non-native libraries. The specific libraries used, along with their corresponding versions, will be duly referenced in the following sections. We believe this level of transparency is crucial to understand the depth and scope of our work.

In the subsequent sections, we will delve into greater detail about our implementation, covering different aspects of our TinyOneM2M solution, and providing a comprehensive view of what was achieved.

\subsection{WebService REST}

The WebService REST module, as discussed in Section \ref{sec:archwebservice}, plays a vital role in our TinyOneM2M solution. It functions as the primary communication hub, receiving \gls{http} requests and handling client connections. This module is pivotal to our architecture, acting as the gateway through which third-party systems and applications interact with our TinyOneM2M solution.

The implementation of this module is carried out with the pthread library for thread creation, and the native socket library for setting up and managing the \gls{rest} Webserver. When the webserver detects new clients, it accepts them and launches a new thread. This thread is responsible for reading the content of the request and executing the complete request processing workflow.

To manage webserver routes, a double-sorted list was employed. This list uses the alphabetical order of the resource path as the key for sorting and simple fields to facilitate resource queries. The resource path, which is a composite of the \texttt{resource name} of the resources in the resource hierarchy, is immutable.

Upon the application's start, the database is scanned to construct the ordered double list, which is subsequently stored in \gls{ram}. This strategy ensures enhanced query efficiency whenever the existence of a request's resource path needs to be verified. When a resource is inserted or deleted, the list is updated accordingly.

However, the use of pthread and the native socket library present a few limitations. While efficient, these libraries may not offer optimal compatibility for systems like Windows, thus limiting the platform independence of our solution. A potential enhancement could be the use of a cross-platform library, such as Drogon \footnote{\url{https://github.com/drogonframework/drogon}} for C++, which could offer non-blocking operations for the \gls{http} server.

It's important to note that the algorithm for managing the double-sorted list was custom implemented by the authors. While this allowed for a tailored solution to our specific needs, a well-tested library could have been utilized for this purpose, potentially offering more robustness and efficiency. Nonetheless, our current implementation is situated within the main process and has shown competent performance in managing routes for the webserver.

\subsection{Response Processor}

As introduced in Section \ref{sec:archresponseprocessor}, the Response Processor module is central to handling the incoming content from the solution's clients. It undertakes critical tasks such as reading the content of the clients' connections, validating the routes based on the sorted double list, and processing the \gls{http} request types.

The module employs a built-in hash table to efficiently map the resources affected by the requests. The hash table provides swift access to key values, making it highly suitable for this task. It inspects and interprets the resource types encapsulated in the \gls{http} requests, facilitating the subsequent processing steps.

One important aspect to note is the static nature of the hash table. Stored in memory, the hash table remains constant for the entire lifetime of the service. This means it is built only once during the service initialization and does not change over time.

However, similar to the WebService \gls{rest} module, the algorithm to manage the hash table was also a custom implementation. Although it is designed to meet the specific needs of this application, a pre-existing, well-tested library could offer enhanced reliability and potentially better performance.

\subsection{M2M Protocol Wrapper}

As established in section \ref{architecture}, the \texttt{M2M Protocol Wrapper} is instrumental for the operation of the entire system. Its functions range from initial protocol initialization, resource discovery, and building values, to defining default values and facilitating the insertion of resources into the database.

Upon protocol initialization, the database is created and the resource CSE\_Base is populated if this has not been done previously. The resource discovery feature is 28\% implemented according to the standard, implementing 9 of the suggested 32 filters.

Table \ref{tab:discovery_attributes} describes each implemented discovery attribute:

\begin{table}[ht]
\scriptsize
\centering
\caption{Description of implemented discovery attributes}
\label{tab:discovery_attributes}
\begin{tabular}{c c p{4cm}}
\hline
\textbf{Filter Name} & \textbf{Cardinality} & \textbf{Description} \\
\hline \hline
resourceType & 0..1 & The \texttt{resourceType} attribute of the matched resource is the same as the specified value. It also allows \texttt{differentiating} between normal and announced resources. \\
labels & 0..1 & The \texttt{labels} attribute of the matched resource matches the specified value. \\
createdBefore & 0..1 & The \texttt{creationTime} attribute of the matched resource is chronologically before the specified value. \\
createdAfter & 0..1 & The \texttt{creationTime} attribute of the matched resource is chronologically after the specified value. \\
modifiedSince & 0..1 & The \texttt{lastModifiedTime} attribute of the matched resource is chronologically after the specified value. \\
unmodifiedSince & 0..1 & The \texttt{lastModifiedTime} attribute of the matched resource is chronologically before the specified value. \\
expireBefore & 0..1 & The \texttt{expirationTime} attribute of the matched resource is chronologically before the specified value. \\
expireAfter & 0..1 & The \texttt{expirationTime} attribute of the matched resource is chronologically after the specified value. \\
filterOperation & 0..1 & Indicates the logical operation (AND/OR) to be used for different condition tags. The default value is logical AND. \\
limit & 0..1 & The maximum number of resources to be included in the filtering result. \\
\hline
\end{tabular}
\end{table}

The module performs minor database queries to build values before their use. It also validates the attributes of the resources and defines default values, such as creating the automatic "Resource Name" when necessary. Furthermore, it builds a \gls{json} structure that gets mapped to a native structure for later insertion into the database by the resource module.

The CJson\footnote{\url{https://github.com/DaveGamble/cJSON}} library (version 1.7.15) is utilized for handling \gls{json} content. Note that this library is used statically, as the source code was imported into this project.

\subsection{M2M Resources}\label{sec:m2mresources}

Our TinyOneM2M solution provides a comprehensive implementation of several critical resources as per the OneM2M standard. These resources -- CSE\_Base, \gls{ae}, \gls{cnt}, \gls{cin} and \gls{sub} -- serve as the backbone of our application, enabling diverse functionalities and providing a robust foundation upon which our solution operates.

The CSE\_Base resource serves as a central hub, managing and organizing various resources and services within our solution. It acts like a system's set of databases, ensuring efficient data management. The \gls{ae} resource encapsulates application-specific information, allowing our solution to adapt to different use cases and applications. The \gls{cnt} and \gls{cin} resources work together to handle and manage information tables and their records, ensuring effective data management. Lastly, the \gls{sub} resource provides real-time monitoring and quick response to changes by acting as watchers that can be added to any of the mentioned resources, including CSE\_Base, \gls{ae}, \gls{cnt}, or even within another \gls{sub}.

As shown in Table \ref{tab:resourceimplementationpercentage}, we have been successful in achieving substantial coverage of the OneM2M standard. Our implementation covers 52\% of the standard for the CSE\_Base resource, 57\% for \gls{ae}, 68\% for \gls{cnt}, 70\% for \gls{cin}, and 40\% for \gls{sub}. These percentages reflect our commitment to aligning our solution as closely as possible with the OneM2M standard, ensuring compatibility and interoperability, while also adapting the solution to meet the specific needs and constraints of our use case.

\begin{table}[h]
\scriptsize
\centering
\caption{Resource Implementation Percentage}
\label{tab:resourceimplementationpercentage}
\begin{tabular}{p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}}
\hline
\textbf{Resource Type} & \textbf{Implemented Attributes} & \textbf{Attributes Suggested by the Standard} & \textbf{Percentage (\%)} \\
\hline \hline
CSE\_Base & 12 & 23 & 52\% \\
AE & 20 & 35 & 57\% \\
CNT & 17 & 25 & 68\% \\
CIN & 14 & 20 & 70\% \\
SUB & 12 & 30 & 40\% \\
\hline
\end{tabular}
\end{table}

The implementation of each feature is based on the attributes suggested in Section 9.6.3 of the OneM2M standard documentation (version 5.1), excluding some less relevant attributes like \texttt{appName}, \texttt{AE-ID}, \texttt{CSE-ID}, etc.

Crucially, the implementation of certain attributes alters the system's normal functioning. All resources implement the expiration time attribute, thus preventing access to any resource whose expiration time is less than the current datetime. For the \gls{cnt} resource, the attributes max byte size, max number of instances, current byte size, and current number of instances were implemented. For the \gls{sub} resource, the event notification criteria attribute was implemented to specify which \gls{http} request methods will trigger notifications.

The standard is broad, and some fields may not be relevant for all users. Hence, we have selected the most pertinent attributes for implementation and earmarked the remainder for potential future work.

\subsection{Embedded Single File Database}

The architecture of our TinyOneM2M solution employs an Embedded Single File Database module, serving as the central data repository. Tasked with persisting resource data, managing hierarchical relationships between resource entities, and enabling efficient data retrieval mechanisms, this module is instrumental in achieving a reliable, efficient, and manageable software system.

For our database management system, we selected SQLite3 \footnote{\url{https://www.sqlite.org/cintro.html}}, an established file-based database known for its performance and simplicity. We integrated the SQLite3 library, specifically version 3.41.1, as source code into our architecture, thereby ensuring broad compatibility across different operating systems. This choice aligns well with our commitment to a platform-independent solution.

SQLite3's incorporation offers numerous advantages, aligning perfectly with our solution's design principles. It is lightweight, efficient, and enables swift database operations with minimal resource usage. Its ease in executing queries contributes directly to our system's responsiveness and performance.

The initial architecture of our database comprised two tables: one for single-valued attributes and another for multi-valued attributes, such as labels and points of access. This design aimed to maintain the structure and initial order of these attributes. However, this approach posed performance challenges during database operations, prompting us to shift towards a denormalized, single-table design to enhance performance. Section \ref{sec:normalizationvsdenormalization} provides a detailed comparison of operations on normalized versus denormalized databases.

While SQLite3 serves our current needs effectively, we are mindful of potential limitations. Alternative embedded database solutions like MySQL or PostgreSQL could prove beneficial under specific conditions such as high concurrency or heightened system requirements. One area for potential enhancement within SQLite3 is the use of cache mode, leveraging RAM for faster operations \footnote{\url{https://www.sqlite.org/inmemorydb.html}}. However, this approach can pose challenges with memory blocking in a multi-threaded environment and requires careful handling \cite{}.

For future development, a detailed analysis of SQLite3 database operations could identify potential optimizations at the attribute level, possibly using the PRAGMA operator. Transactional behavior could also be improved by exploring write-ahead logging (WAL) to enhance concurrency. Particularly in a multi-threaded environment, concurrent reads and writes can be significantly optimized with WAL, providing a valuable addition to our implementation \cite{}.

\subsection{Notifications}

The Notifications module plays an integral role in our TinyOneM2M solution, responsible for alerting subscribers about changes in resources they have subscribed to. Notifications, often generated through modifications in resources, are essential to ensure subscribers stay informed about the current state of resources they are interested in.

Our notification system is built around the subscription resource feature. Whenever a notification needs to be dispatched, a new thread is spun up to handle the delivery process. This thread-centric approach enables our system to maintain a high level of responsiveness and efficiency by preventing the blocking of main execution threads. A subscription in TinyOneM2M serves as a contractual agreement between a subscriber and a resource. Whenever a resource that a client has subscribed to undergoes any change, a notification is triggered. This subscription-driven approach ensures that notifications are contextually relevant and valuable to the subscriber.

To facilitate the creation of \gls{mqtt} and/or \gls{http} clients within these notification threads, we have utilized the Mongoose library \footnote{\url{https://github.com/cesanta/mongoose/tree/master}} (version 7.10). The use of this library greatly simplifies the process of setting up clients and managing connections, contributing to the overall efficiency and reliability of our notification system.

The delivery addresses for notifications are defined within the \texttt{nu} attribute in the subscription. For example, the delivery addresses could be specified as \texttt{mqtt://0.0.0.0:1883, http://0.0.0.0:1400/monitor}. Similarly, the topic or resource path, such as \texttt{/onem2m/ae-test/cnttest/subtest}, denotes the address of the resource associated with the notification.

Further enhancing the specificity of our notification system, we've implemented an attribute called event notification criteria, as described in Section \ref{tinyonem2marchitecture}. This attribute allows specifying which \gls{http} request methods should trigger notifications. For instance, setting this attribute to \texttt{"GET, POST"} ensures that notifications are triggered only when \gls{http} GET or POST requests affecting direct child resources occur. This gives subscribers precise control over which actions result in notifications, thus providing a highly customizable notification experience. 

For example, if a container resource contains a subscription, a POST request to create a content instance resource within this container will trigger a notification. However, a DELETE request will not lead to a notification, as the DELETE method is not included in the event notification criteria. This flexibility provides precise control over notification triggers, tailoring notifications based on subscribers' specific interests or system requirements.

\section{Tests}
\label{tests}

In this crucial section, we delve into the testing mechanisms employed to examine the TinyOneM2M solution under various scenarios, highlighting its strengths and exposing areas for improvement.

Firstly, the Database Normalisation versus Database Denormalisation test will provide a performance comparison of operation execution time between normalized and denormalized databases. The purpose of this test is to unveil any potential performance ramifications of the chosen database structure.

Our second test is dedicated to Assessing the Efficiency of Meeting Expiration Time Attribute Objectives in Database Queries on Performance. Here, we focus on the expiration time attribute, which either hides or eliminates records with an expiration time value surpassing the current date time. We aim to contrast the duration of database operations with and without the implementation of this attribute, focusing on the average time spent on each operation type.

A crucial part of our testing process is the Comparative Analysis of Latency for CRUD operations using both our proposed solution and OpenMTC. This test involves a direct performance comparison between our TinyOneM2M implementation, adhering to version 5.1, and OpenMTC, which complies with the standard version 1.1. We meticulously measure the average time expended on the Create, Read, Update, and Delete operations.

In the Cross-Platform Evaluation, we demonstrate the adaptability of the TinyOneM2M system by evaluating its ease of installation and compilation on two distinct operating systems: Ubuntu and macOS. The test showcases the platform-independent nature of our solution, adding to its versatility.

Lastly, our stress test puts the TinyOneM2M solution through rigorous trials on the Raspberry Pi 0 W microcontroller under the Minimal Hardware Stress Testing test. This test is intended to simulate high-pressure scenarios involving a large number of insertion requests to examine the resilience of our solution.

We've conducted these tests on three machines of varying specifications, each listed in Table \ref{tab:machinesSpecs}. The diversity in our testing apparatus reiterates the solution's flexibility across an array of hardware and software configurations. Notably, the libraries deployed on each machine, along with their respective versions, remain static across the board as outlined in Section \ref{sec:tinyonem2msolution}.

Table \ref{tab:machinesSpecs} delivers a comprehensive overview of the specific hardware, operating system, compiler, and library configurations for the machines used in our tests. This information is instrumental for anyone aiming to replicate the testing environment and offers valuable context to our results.

\begin{table}[h]
	\scriptsize
	%\makebox[\textwidth]{
	\caption{Hardware, Operating System, Compiler, and Library Specifications for Test Bed Machines.}
	\label{tab:machinesSpecs}
	\begin{tabular}{>{\bfseries}p{3cm} p{4cm}}
	\toprule
	\multicolumn{2}{c}{\textbf{Machine1}} \\
	\toprule
	\toprule
	\multicolumn{2}{c}{\textbf{Hardware}} \\
	\midrule
	Chip                & Apple M1 Pro @ 3.2 GHz \\
	Total Number of Cores & 10 (8 performance and 2 efficiency) \\
	RAM Memory          & 16 GB \\
	\midrule
	\multicolumn{2}{c}{\textbf{Operating System}} \\
	\midrule
	Product Name         & macOS \\
	Product Version      & 12.6 \\
	Kernel Version        & Darwin 21.6.0 \\
	\midrule
	\multicolumn{2}{c}{\textbf{Compiler}} \\
	\midrule
	GCC Version         & Apple clang version 14.0.0 (clang-1400.0.29.102) \\
	\bottomrule
	
	\toprule
	\multicolumn{2}{c}{\textbf{Machine2}} \\
	\toprule
	\toprule
	\multicolumn{2}{c}{\textbf{Hardware}} \\
	\midrule
	Chip                & Intel(R) Core(TM)2 CPU 6600 @ 2.40GHz \\
	Total Number of Cores & 2 \\
	RAM Memory          & 8 GB \\
	\midrule
	\multicolumn{2}{c}{\textbf{Operating System}} \\
	\midrule
	Product Name         & Ubuntu \\
	Product Version      & 20.04.5 \\
	Kernel Version        & 5.15.0-67-generic \\
	\midrule
	\multicolumn{2}{c}{\textbf{Compiler}} \\
	\midrule
	GCC Version         & Ubuntu 9.4.0-1ubuntu1~20.04.1 \\
	\midrule
	\bottomrule
	
	\toprule
	\multicolumn{2}{c}{\textbf{Machine3}} \\
	\toprule
	\toprule
	\multicolumn{2}{c}{\textbf{Hardware}} \\
 	\midrule
	Chip                & BCM2835 @ 1GHz ARMv6 \\
	Total Number of Cores & 1 \\
	RAM Memory          & 512MB \\
	\midrule
	\multicolumn{2}{c}{\textbf{Operating System}} \\
	\midrule
	Product Name         & Debian \\
	Product Version      & 12.6.3 \\
	Kernel Version       & Darwin 21.6.0 \\
	\midrule
	\multicolumn{2}{c}{\textbf{Compiler}} \\
	\midrule
	GCC Version         & Debian 8.3.0-6 \\
	\bottomrule

	\end{tabular}
\end{table}

\subsection{Database Normalization VS Database Denormalization} \label{sec:normalizationvsdenormalization}

In this section, we present an investigation into the performance of database normalization versus denormalization. This analysis does not directly implement the OneM2M standard but instead provides a comparative assessment of these two database structuring strategies.

To execute the tests, we employed a Python script directly on the SQLite3 database. The script uses the SQLite3 library and was run on Machine 1. The version of Python used was 3.9.10 and the version of the SQLite3 library used was 3.37.0. The SQLite3 database was allocated in the RAM.

\begin{algorithm}
\scriptsize
\caption{Pseudocode to compare SQLite Performance with Hierarchy Complexity}
\label{alg:dbstrategycomparison}
\begin{algorithmic}[1]
\Procedure{[Insert/Select/Update/Delete]Data}{conn, table, depth}
    \State $\text{start time}$
    \For{$i=1$ to $\text{depth}$}
        \State $\text{execute SQL to [Insert/Select/Update/Delete] data in table with hierarchy depth i}$
    \EndFor
    \State $\text{end time}$
    \State $\text{calculate and print time difference divided by depth}$
\EndProcedure
\\
\Procedure{ComparePerformance}{}
    \State $\text{conn} \gets \text{open database connection}$
    \State $\text{tables} \gets \text{[OneM2M\_Denorm, CSE\_Base\_Norm, AE\_Norm, CNT\_Norm, SUB\_Norm]}$
    \State $\text{depth} \gets \text{15}$
    \State $\text{CreateDenormalizedTables(conn)}$
    \State $\text{CreateNormalizedTables(conn)}$
    \For{each $\text{table}$ in $\text{tables}$}
        \State $\text{InsertData(conn, table, depth)}$
        \State $\text{SelectData(conn, table, depth)}$
        \State $\text{UpdateData(conn, table, depth)}$
        \State $\text{DeleteData(conn, table, depth)}$
    \EndFor
    \State $\text{close database connection}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

To execute the tests, we employed a Python script directly on the SQLite3 database. A pseudocode representation of the script used can be found in Algorithm \ref{alg:dbstrategycomparison}. The script performs Insert, Select, Update, and Delete operations on tables of different hierarchical depths, calculating the average operation time per depth. The depth used for these operations was 15, and each operation was performed 1000 times for each database. The structures of the normalized and denormalized databases are depicted in Figure \ref{fig:TestComparisionDBsStrategies} mainly showing the hierarchy of resources.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{TestComparisionDBsStrategies}
	\caption{Diagram of Database Structure used for Comparison of the Normalized Strategy and Denormalized Strategy.}
	\label{fig:TestComparisionDBsStrategies}
\end{figure}

To analyze the results, let's refer to Table \ref{tab:DBComparisonResults}. For the insertion operation, the denormalized structure was slightly more time-efficient, taking an average of $6.96 \times 10^{-6}$ seconds per depth layer to insert a record, while the normalized structure ranged between $6.82 \times 10^{-6}$ and $7.23 \times 10^{-6}$ seconds per depth layer.

The selection operation showed a notable difference between the two database structures. Simple read operations on the denormalized structure consumed significantly more time (around $0.0045$ seconds) than the normalized structure. However, with deep querying, the normalized structure showed an increase in operation time (up to $0.0033$ seconds) due to the complexity of join operations.

In the case of the update operation, the normalized structure was slightly quicker, taking about $6.77 \times 10^{-6}$ to $7.32 \times 10^{-6}$ seconds per depth layer compared to $6.90 \times 10^{-6}$ seconds in the denormalized structure. Similar to the update operation, the delete operation was slightly faster in the normalized structure, taking between $3.67 \times 10^{-6}$ and $4.01 \times 10^{-6}$ seconds per depth layer, while the denormalized structure took about $3.60 \times 10^{-6}$ seconds.

\begin{table}[h]
\scriptsize
\centering
\caption{Average time per depth layer for CRUD operations on the databases}
\label{tab:DBComparisonResults}
\begin{tabular}{ p{1cm} p{3cm} p{3cm} }
\hline
\textbf{Operation} & \textbf{Denormalized Structure Seconds} & \textbf{Normalized Structure Seconds} \\
\hline \hline
Insert & $6.96 \times 10^{-6}$ (s) & $6.82 \times 10^{-6}$ to $7.23 \times 10^{-6}$ (s) \\
Select & $0.0045$ (s) & $1.33 \times 10^{-6}$ to $0.0033$ (s) \\
Update & $6.90 \times 10^{-6}$ (s) & $6.77 \times 10^{-6}$ to $7.32 \times 10^{-6}$ (s) \\
Delete & $3.60 \times 10^{-6}$ (s) & $3.67 \times 10^{-6}$ to $4.01 \times 10^{-6}$ (s) \\
\hline
\end{tabular}
\end{table}

From these results, we observe that the denormalized structure was more efficient in terms of insert operations, a crucial factor for applications necessitating large volumes of data insertions. However, when it comes to deep querying in a hierarchical structure, the denormalized structure outperforms due to the overhead of join operations in normalized tables. While these results are based on a relatively small example and an embedded database system, it is expected that as the database scales, these differences could become more pronounced.

\subsection{Assessing the Efficiency of Meeting Expiration Time Attribute Objectives in Database Queries on Performance}

In this section, we focus on a specific test case that aims to assess the efficiency of meeting expiration time field objectives in database queries on performance, using codebase version 1.1. The database contains 2000 records in the "mtc" table and 41,000 in the "multivalue" table, adding difficulty to the SELECT queries. The "expiration time" field's purpose is to ensure that resources are deleted or become unused after their expiration. The test will be performed on a "discovery" request with five parameters: ty (resource type), lbl (labels), expireAfter using the "expiration time" field, expireBefore using the "expiration time" field, and filterOperation. It is important to note that the "labels" attribute is an array and its search requires more effort because it searches in the "multivalue" table. By default, the "discovery" request will have a limit of 50 rows listed, which can be adjusted as a parameter, but for this test, the default value will be used. As the limit value increases, the query's latency may grow exponentially, making it crucial to optimize the database and query structure. Three distinct approaches were considered, with the first one being the primary focus of this study:

\begin{itemize}
	\item Selective Query Filtering - This approach filters out expired records by adding a condition to consultation, update, and removal queries to apply only to records with a valid "expiration time" field. A REST route is created to delete invalid records. This method is advantageous because it minimizes the overhead associated with constantly checking and deleting expired records while still maintaining data integrity. The performance of this approach will be compared with two other scenarios in terms of query time.
	\item On-demand Record Deletion - In this approach, all requests use the current context to search for records whose "expiration time" field has already passed and, if so, deletes them. This method adds complexity to each request, potentially impacting performance and increasing the likelihood of implementation errors.
	\item Periodic Expiration Check - A thread is created that remains active indefinitely, checking every X time if there are fields with an exceeded "expiration time". However, there is a chance that expired records would still be present when queries are made on the main threads. This approach can lead to additional resource consumption and potential race conditions, making it less ideal than the first approach.
\end{itemize}

The test case under examination focuses on the first approach, "Selective Query Filtering," and examines its performance in three different scenarios using codebase version 1.1: the codebase with the "expiration time" field as a condition in database queries, the codebase with the "expiration time" field as a condition in database queries when it is indexed in the database, and the original codebase without the "expiration time" field as a condition. The test will be performed by executing 500 "discovery" requests with five parameters in each of the three scenarios on "Machine 1", as shown in Table \ref{tab:machinesSpecs}. By evaluating the efficiency of these scenarios in meeting the objectives of the "expiration time" field, we can identify the most effective method for ensuring proper resource management.

The first approach, "Selective Query Filtering," has been chosen as the primary focus of this study due to its potential for minimizing overhead while maintaining data integrity. The comparative analysis of the three scenarios will provide valuable insights into the impact of different handling techniques for the "expiration time" field on system performance using codebase version 1.1. This assessment will help identify potential optimizations and guide the selection of the most suitable approach for managing resources with an expiration time, ultimately enhancing the overall efficiency of the system.

To assess the performance of the three scenarios in the test case, we have designed a simple algorithm that sends a total of 500 requests, each retrieving data through SELECT queries to the target route for each scenario, considering the five parameters mentioned above. This algorithm measures the time taken for each request in seconds and accumulates the minimum, maximum, and total time. Additionally, it calculates the squared difference of each request's time and the average time to determine the standard deviation. By using this algorithm, we can consistently evaluate the time efficiency of the "Selective Query Filtering" approach in comparison to the other scenarios. The pseudocode for this algorithm is shown in Algorithm \ref{alg:time_measurement}.

\begin{algorithm}
\caption{Pseudocode for Time Measurement}\label{alg:time_measurement}
\begin{algorithmic}[1]
\State Initialize $\textit{count}$, $\textit{sum}$, $\textit{min}$, $\textit{max}$, $\textit{sq\_diff\_sum} \gets 0$
\State $\textit{min} \gets$ large value
\For{$i \gets 1$ to $500$}
    \State Send a request to the target route
    \State Measure the time taken for the request ($\textit{time\_ms}$)
    \If{$\textit{time\_ms} < \textit{min}$}
        \State $\textit{min} \gets \textit{time\_ms}$
    \EndIf
    \If{$\textit{time\_ms} > \textit{max}$}
        \State $\textit{max} \gets \textit{time\_ms}$
    \EndIf
    \State $\textit{sum} \gets \textit{sum} + \textit{time\_ms}$
\EndFor
\State Calculate $\textit{average} \gets \frac{\textit{sum}}{500}$
\For{$i \gets 1$ to $500$}
    \State $\textit{diff} \gets \textit{time taken for request i} - \textit{average}$
    \State $\textit{sq\_diff} \gets \textit{diff} \times \textit{diff}$
    \State $\textit{sq\_diff\_sum} \gets \textit{sq\_diff\_sum} + \textit{sq\_diff}$
\EndFor
\State Calculate $\textit{standard\_deviation} \gets \sqrt{\frac{\textit{sq\_diff\_sum}}{500}}$
\end{algorithmic}
\end{algorithm}

With the algorithm in place, we can proceed to the actual testing phase. For each of the three scenarios, we will run the algorithm on "Machine 1" and record the minimum, maximum, average, and standard deviation of the request times. This data will allow us to thoroughly analyze the performance impact of each approach and draw conclusions about their efficiency in meeting the "expiration time" field objectives. The results of the tests are presented in Table \ref{tab:performance_comparison}.

\begin{table*}[htbp]
\centering
\caption{Performance comparison of the three scenarios}
\begin{tabular}{lcccc}
\hline
Scenario & Min (s) & Max (s) & Avg (s) & Std Dev (s) \\ \hline
Original Codebase without Expiration Time Condition & 0.614603 & 0.683479 & 0.626082 & 0.007875 \\
Non-Indexed Expiration Time Condition & 0.614021 & 0.672247 & 0.622916 & 0.007017 \\
Indexed Expiration Time Condition & 0.629939 & 1.446011 & 0.640483 & 0.045949 \\ \hline
\end{tabular}
\label{tab:performance_comparison}
\end{table*}

Upon examining the results in Table \ref{tab:performance_comparison}, we can derive several insights into the efficiency of the "Selective Query Filtering" approach in managing resources with an expiration time. These insights will be valuable in guiding future optimizations and ensuring proper resource management within the system.

It is important to highlight the trade-offs when comparing the three scenarios. First, the original codebase without the expiration time condition exhibits slightly better average performance compared to the indexed expiration time condition scenario. This result is expected, as the original codebase does not include any additional checks for expired resources, which results in reduced complexity and faster query times. However, the difference in average time between the original codebase and the non-indexed expiration time condition scenario is minimal, indicating that the added complexity of checking for expiration does not significantly impact the overall performance.

When comparing the non-indexed expiration time condition scenario with the indexed expiration time condition scenario, we observe a slight increase in the average time for the indexed case, which might seem counterintuitive as indexing usually improves query performance. However, it is essential to consider that the standard deviation for the indexed expiration time condition scenario is considerably higher, indicating more varied response times. This variability could be attributed to the database's specific structure, the distribution of data within the tables, or the types of queries executed during the test. Despite the higher average time, the indexed expiration time condition scenario offers a more efficient approach to managing resources with an expiration time. The increased standard deviation highlights the variability in response times, which can be attributed to various factors such as the database's structure or the nature of the queries executed during the test. Consequently, the indexed scenario demonstrates greater scalability and adaptability in handling larger volumes of data, making it the preferred choice for implementation.

As the system evolves, the indexed expiration time condition scenario will be the one that prevails and will be part of the solution implementation. It is recommended to further investigate the factors contributing to the variability in response times for this scenario and explore the performance impact of different limit values. This analysis will help guide future optimizations and enhance the system's overall efficiency while meeting the expiration time field objectives.

Furthermore, it is essential to note that the "discovery" request route being tested involves five parameters, which add complexity to the evaluation of each scenario's performance. As the limit value of 50 rows listed increases, the query's latency may grow exponentially. Therefore, the current test with a default limit of 50 rows serves as a starting point, and it is crucial to evaluate the performance of these scenarios with varying limit values to understand their impact on the overall system efficiency.

In conclusion, the "Selective Query Filtering" approach has shown promise in terms of meeting the expiration time field objectives with a minimal impact on database query performance. The small performance difference between the original codebase without the expiration time condition and the non-indexed expiration time condition scenario indicates that checking for expiration does not drastically impact performance. It is recommended to investigate the indexed expiration time condition scenario further to understand the variability in its response times and to explore the performance impact of different limit values. This analysis will guide future optimizations and help select the most suitable approach for managing resources with an expiration time, ultimately enhancing the system's overall efficiency.

\subsection{Comparative Analysis of Latency for CRUD Operations Using the Proposed Solution and OpenMTC}

In this section, we delve into a detailed test case that focuses on the comparative analysis of latency for CRUD (Create, Read, Update, and Delete) operations using the proposed solution and OpenMTC. The objective of this test case is to evaluate the performance of our solution in managing resources against the established OpenMTC standard, providing a benchmark for assessing the efficiency and effectiveness of our approach.

The CRUD operations will be performed through REST requests over the network, rather than on a local machine, as the primary goal of the oneM2M gateway is to facilitate the exchange of information across a distributed network infrastructure. This setup simulates real-world conditions where gateway systems must efficiently handle remote requests, thereby providing a more accurate representation of the solution's performance in practical applications.

To carry out this test case, we will use "Machine 2", as shown in Table \ref{tab:machinesSpecs} as described in the previous test case, with the following specifications: Apple M1 Pro chip, 10 cores, 16 GB RAM, and macOS version 12.6. The codebase version for the proposed solution is 1.2. The test will be conducted using a shell script that sends 500 Application Entity (AE) resource CRUD operations for both the proposed solution and the OpenMTC platform.

The shell script operates as follows: It begins by initializing variables and CSV files for each CRUD operation (POST, PUT, GET, and DELETE). Then, it iterates 500 times, performing the following steps in each iteration: sending a POST, PUT, GET, and DELETE request to the respective servers, measuring the time taken for each request, and appending the time to the corresponding CSV file. After completing the 500 iterations, the script calculates the minimum, maximum, average, and standard deviation of the times for each CRUD operation from their respective CSV files. The results of these calculations provide insights into the latency of the proposed solution and OpenMTC for each CRUD operation.

By comparing the latency for 500 Application Entity (AE) resource CRUD operations on both platforms, we can highlight the strengths and areas for improvement of our proposed solution. This analysis will not only help us to understand the impact of our solution on system performance but also to identify any potential bottlenecks and areas that could benefit from optimization, particularly in the context of network latency and resource management.

The insights gained from this test case will be crucial in the ongoing development and refinement of our solution, ultimately contributing to a more robust and efficient system for managing resources in the context of the oneM2M standard while accounting for the network-related challenges inherent in distributed systems.

\subsection{Cross-Platform Evaluation: Installation and Compilation on Ubuntu and macOS Systems}

In this section, we introduce a test case aimed at evaluating the cross-platform compatibility of our proposed solution through installation and compilation on two widely-used operating systems, Ubuntu and macOS. The objective of this test case is to ensure that our solution can be seamlessly integrated into a variety of operating environments, thereby demonstrating its versatility and adaptability to different system configurations.

A cross-platform evaluation is crucial in the realm of computer science, as it verifies the portability and interoperability of software across different platforms. By validating our solution's compatibility with both Ubuntu and macOS, we demonstrate its ability to function effectively in diverse contexts, catering to a broader range of users and potential applications.

This test case will involve the installation of necessary dependencies, compilation of the source code, and execution of the proposed solution on both operating systems. The evaluation will not only focus on the successful completion of these tasks but also on identifying any platform-specific issues, discrepancies, or bottlenecks that may arise during the process.

The insights gained from this cross-platform evaluation will contribute to the ongoing refinement of our solution, ensuring that it remains robust and effective across various operating environments. This, in turn, enhances its potential for widespread adoption and application in the context of the oneM2M standard.

In this test case, we successfully installed and compiled the proposed solution on Ubuntu, one of the two operating systems being evaluated for cross-platform compatibility. The process began by importing the source code to "Machine 2", as shown in Table \ref{tab:machinesSpecs}, using SFTP, since the operation system have already installed the GCC compiler theres no more dependencies to install and the system is ready to be compiled.

The proposed solution was successfully compiled using the provided Makefile, which facilitated the creation of the executable without any issues. A closer examination of the dependencies of the compiled executable was carried out using the \texttt{ldd} command. The command revealed that the proposed solution relies exclusively on system libraries such as 'libdl.so.2', 'libpthread.so.0', and 'libc.so.6', which provide essential functionality for dynamic loading, multi-threading, and standard C functions, respectively.

By relying solely on system libraries, the proposed solution demonstrates a significant advantage in terms of portability and maintainability. This approach eliminates the need for third-party dependencies, reducing potential compatibility issues and simplifying the installation process across various platforms. Furthermore, it ensures that the solution benefits from the stability and performance optimizations provided by the operating system's native libraries, which are generally well-maintained and frequently updated.

The successful installation, compilation, and identification of dependencies on Ubuntu demonstrate that the proposed solution is adaptable to various operating environments. This adaptability is essential in the realm of computer science as it ensures the portability and interoperability of the software across different platforms. By validating our solution's compatibility with Ubuntu, we have shown its ability to cater to a broader range of users and potential applications.

As the next step in the cross-platform evaluation, the proposed solution will be installed and compiled on macOS. This process will involve identifying and installing any necessary dependencies, compiling the source code, and executing the solution on this operating system.

In this test case, the proposed solution was successfully installed and compiled on the macOS operating system. The source code was imported to "Machine 1", and the necessary dependency, GCC, was found to be already installed on the system. The provided Makefile facilitated the compilation of the files, resulting in the generation of the executable without any issues.

The examination of the dependencies of the compiled executable revealed that the proposed solution relies exclusively on system libraries. The \texttt{otool} command showed that the solution depends on '/usr/lib/libSystem.B.dylib', which provides essential functionality for dynamic loading, memory allocation, and system calls. By relying solely on system libraries, the proposed solution demonstrates a significant advantage in terms of portability and maintainability, as it eliminates the need for third-party dependencies.

The use of system libraries highlights the proposed solution's adaptability to different system configurations, ultimately enhancing its potential for widespread adoption and application in the context of the oneM2M standard. This approach ensures that the solution benefits from the stability and performance optimizations provided by the operating system's native libraries, which are generally well-maintained and frequently updated.

By successfully installing and compiling the proposed solution on both Ubuntu and macOS, we have demonstrated its ability to function effectively in diverse contexts, catering to a broader range of users and potential applications. The insights gained from this cross-platform evaluation will inform any necessary adjustments or refinements to ensure seamless integration into various operating environments.

\textcolor{red}{...Final Thoughts...}

\subsection{Minimal Hardware Compatibility Testing: Raspberry Pi 0 W Case Study}

In this section, we present a test case focused on assessing the compatibility of our proposed solution with minimal hardware, specifically using the ESP32 microcontroller as a case study. The objective of this test case is to verify the suitability of our solution for resource-constrained environments, thereby broadening its potential applications and enabling its adoption in a wider range of use cases.

Minimal hardware compatibility is an essential aspect of computer science, as many real-world applications require efficient and lightweight solutions that can run on devices with limited processing power, memory, and energy resources. By evaluating the performance of our proposed solution on the ESP32, a popular and cost-effective microcontroller with integrated Wi-Fi and Bluetooth capabilities, we can demonstrate its ability to function effectively under such constraints.

This test case will involve the adaptation, deployment, and execution of our solution on the ESP32 platform. The evaluation will not only focus on the successful completion of these tasks but also on identifying any hardware-specific issues, discrepancies, or bottlenecks that may arise during the process. Additionally, we will assess the resource usage, including memory and processing overhead, to ensure that the solution remains viable and efficient in resource-constrained scenarios.

The insights gained from this minimal hardware compatibility testing will contribute to the ongoing refinement of our solution, ensuring that it remains both robust and versatile, capable of meeting the demands of a diverse range of applications in the context of the oneM2M standard.






%Avaliação do impacto resultante do cumprimento do objetivo do campo expiration time
%
%Especificações da máquina da escola
%Especificações da rede
%Meter o executável a funcionar na máquina com o ficheiro de configuração
%
%Shell script para executar 500 POST, guardar minimo, maximo, média, desvio padrão
%Shell script para executar 500 PUT, guardar minimo, maximo, média, desvio padrão
%Shell script para executar 500 GET, guardar minimo, maximo, média, desvio padrão
%Shell script para executar 500 DELETE, guardar minimo, maximo, média, desvio padrão
%
%
%Quanto tempo demora a POST/PUT/GET/DELETE 500 AEs usando a nossa solução e a solução disponível do OpenMTC na mesma máquina (na maquina do MEI-CM) na rede - Saber o tempo de cada um dos 500 e calcular, minimo, maximo, média, desvio padrão
%
%Instalação e Compilação em máquinas Ubuntu e MacOS (verificar se corre em outros SOs)
%
%? Fazer testes com Hardware minimo - por exemplo ESP32 ?

\section{Lessons Learned}
\label{sec:lessonsLearned}

Over the implementation of TinyOneM2M, our project has encountered numerous challenges and valuable insights. Therefore, this section aims to provide an overview of the lessons learned throughout the project. 

During the development stage, we opted for a single file database over an in-memory database, believing it would offer superior performance. However, as we progressed, we discovered that this assumption might not hold. The performance gains we expected did not materialize, prompting us to reevaluate our choice of database technology and explore alternative options, such as an in-memory database.

Following established database best practices, we initially pursued a normalization approach for our database structure which involved creating two tables: "mtc" and "multivalue." The introduction of the multivalue table played a vital role in our database structure since it facilitated the mapping of intricate dependency relationships between each \gls{m2m} resource. This approach proved particularly valuable in handling the dynamic and complex nature of \gls{json} relationships, enabling us to effectively represent the infinite-like associations that can exist within \gls{json} values. While this approach adhered to standard guidelines, it downgrades performance due to the time-consuming processing required to map a \gls{json} structure in C into corresponding database entries significantly impacting the overall performance of our system. Consequently, we recognized that denormalization would be a more suitable strategy for our project, allowing for improved performance and streamlined data retrieval. Therefore, this ultimately became the final approach we chose to implement our database.

Another of the lessons we learned relates to using static C libraries. While these libraries offer certain advantages, such as ease of distribution and reduced dependency management, they pose challenges when keeping up-to-date with new versions. On the positive side, static libraries provide a stable and predictable development environment. However, the downside is that they do not benefit from bug fixes, security patches, or new features introduced in subsequent releases. This trade-off necessitates careful consideration and evaluation of the specific project requirements when choosing between static and dynamic libraries.

Implementing the OneM2M standard involved incorporating various attributes, including expiration time (et), maximum byte size (mbs), and maximum number of instances (mni), among others. While these attributes align with the standard, their inclusion increases implementation complexity and hinders performance. Moreover, in practical scenarios, these fields often have little relevance or are not frequently utilized. Therefore, the attributes to iplement must be extensively evaluated to strike a balance between adherence and practicality.

\section{Conclusion and Future Work}
\label{conclusion}
\printcredits

%% Loading bibliography style file
%\bibliographystyle{model1-num-names}
%\bibliographystyle{cas-model2-names}
\bibliographystyle{unsrt} % Estilo de Bibliografia
% Loading bibliography database
\bibliography{cas-refs}


%\vskip3pt

\bio{figs/bioFig}
My name is Rafael Pereira, I'm 22 years old, and I'm an MSc Computer Engineering Student and Researcher at Polytechnic of Leiria. I'm extremely curious about the technology world, ambitious for knowledge in this area, I'm dedicated to doing my work. The programming thing always got me, and every day it grows. Today I'm looking for interesting projects, that can make me think and learn every day.
\endbio

\end{document}
